{"version":3,"sources":["PublicComponent/Header.js","PublicComponent/Footer.js","PublicComponent/FailImage.js","Main/AboutMe.js","Main/MainAlertArea.js","Main/MainPage.js","Posts/PostsMain.js","Notes/NotesMain.js","PublicComponent/PageHeader.js","Posts/MyPosts/HowDoNeuralNetworkWork.js","Posts/MyPosts/WhatIsLSTM.js","Posts/MyPosts/ResidualNetwork.js","Posts/MyPosts/WhatIsBayesNetwork.js","Router.js","App.js","reportWebVitals.js","index.js"],"names":["Header","Layout","AppHeader","props","style","position","zIndex","width","padding","className","theme","mode","defaultSelectedKeys","select","Item","to","Footer","AppFooter","textAlign","failImage","Link","Typography","Title","Paragraph","AboutMe","backgroundColor","level","size","align","height","src","process","fallback","copyable","MailOutlined","href","ZhihuOutlined","MainAlertArea","state","AlertList","message","description","type","showIcon","this","setState","length","SetEmptyArea","direction","React","Component","Content","Text","MainContent","window","scrollTo","marginTop","margin","strong","ApiOutlined","orientation","MainPost","closable","title","extra","PlusOutlined","color","ellipsis","rows","expandable","MainNotes","gutter","span","bordered","hoverable","FilePdfOutlined","fontSize","BookOutlined","AppPageHeader","onBack","history","back","subTitle","PhotoLink","HowDoNeuralNetworkWork","PostContent","math","FailImage","language","lightfair","children","WhatIsLSTM","alt","ResidualNetwork","minWidth","class","WhatIsBayesNetwork","BasicRoute","exact","path","component","App","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"sTAQQA,EAAWC,IAAXD,OAqBOE,MAnBf,SAAmBC,GACf,OACI,eAACH,EAAD,CAAQI,MAAO,CAAEC,SAAU,QAASC,OAAQ,EAAGC,MAAO,OAAQC,QAAQ,KAAtE,UACE,qBAAKC,UAAU,SACf,eAAC,IAAD,CAAMC,MAAM,QAAQC,KAAK,aAAaC,oBAAqB,CAACT,EAAMU,QAAlE,UACE,cAAC,IAAKC,KAAN,UACE,cAAC,IAAD,CAAMC,GAAG,IAAT,mBADa,KAGf,cAAC,IAAKD,KAAN,UACE,cAAC,IAAD,CAAMC,GAAG,SAAT,oBADa,KAGf,cAAC,IAAKD,KAAN,UACE,cAAC,IAAD,CAAMC,GAAG,SAAT,oBADa,YCdpBC,EAAUf,IAAVe,OASQC,MAPf,WACI,OAAQ,eAACD,EAAD,CAAQZ,MAAO,CAAEc,UAAW,UAA5B,wBACe,uBADf,qD,oCCRGC,EAFG,iqGCSVC,EAA2BC,IAA3BD,KAAME,EAAqBD,IAArBC,MAAOC,EAAcF,IAAdE,UAiCNC,MA9Bf,WACI,OACI,eAAC,IAAD,CAAQpB,MAAO,CAAEqB,gBAAiB,QAASjB,QAAS,KAApD,UACI,cAACc,EAAD,CAAOI,MAAO,EAAd,sBAEA,eAAC,IAAD,CACIC,KAAK,QACLC,MAAM,QAFV,UAKA,cAAC,IAAD,CACIrB,MAAO,IACPsB,OAAQ,IACRC,IAAG,UAfDC,YAeC,eACHC,SAAUb,IAGd,eAACI,EAAD,WACA,cAACD,EAAD,CAAOI,MAAO,EAAd,yBACI,eAACH,EAAD,CAAWU,UAAQ,EAAnB,UAAoB,cAACC,EAAA,EAAD,IAApB,+BAFJ,UAGW,cAACd,EAAD,CAAMe,KAAK,wCAAX,mDACP,uBACA,cAACC,EAAA,EAAD,IALJ,gBAKiC,cAAChB,EAAD,CAAMe,KAAK,kDAAX,uE,iDCoB9BE,E,4MAvCXC,MAAQ,CACJC,UAAY,CACR,cAAC,IAAD,CACAC,QAAQ,UACRC,YAAY,yCACZC,KAAK,UACLC,UAAQ,IAGR,cAAC,IAAD,CACAH,QAAQ,cACRC,YAAY,sFACZC,KAAK,OACLC,UAAQ,M,6DAKZC,KAAKC,SACD,CACIN,UAAY,cAAC,IAAD,CAAOE,YAAY,gB,+BAQvC,OAHoC,IAAhCG,KAAKN,MAAMC,UAAUO,QACrBF,KAAKG,eAGL,cAAC,IAAD,CACIC,UAAU,WACV5C,MAAO,CAACG,MAAO,QAFnB,SAIKqC,KAAKN,MAAMC,gB,GAlCAU,IAAMC,WCD3BC,EAAWlD,IAAXkD,QACA7B,EAA0BD,IAA1BC,MAAOC,EAAmBF,IAAnBE,UAAW6B,EAAQ/B,IAAR+B,KA4CVC,EAzCK,WAEhB,OADAC,OAAOC,SAAS,EAAE,GAEd,eAAC,IAAD,WACA,cAAC,EAAD,CAAW1C,OAAO,MAClB,eAACsC,EAAD,CAAS1C,UAAU,cAAcL,MAAO,CAAEI,QAAS,SAAUgD,UAAW,IAAxE,UAEE,eAAC,IAAD,CAAYpD,MAAO,CAAEqD,OAAQ,UAA7B,UACE,cAAC,IAAW3C,KAAZ,mBACA,cAAC,IAAWA,KAAZ,2BAGF,sBAAKL,UAAU,yBAAyBL,MAAO,CAAEI,QAAS,IAA1D,UACE,cAAC,EAAD,IAEA,cAAC,IAAD,IAEA,cAAC,EAAD,CAAOkB,MAAO,EAAd,4BALF,+GAQE,uBAEA,cAAC0B,EAAD,CAAMM,QAAM,EAAZ,6CACA,cAAC,EAAD,CAAWzB,UAAQ,EAAnB,6DAEA,eAAC,IAAD,CAAQS,KAAK,UAAUP,KAAK,kDAA5B,UACI,cAACwB,EAAA,EAAD,IADJ,wBAIA,cAAC,IAAD,CAASC,YAAY,OAArB,kCAGA,cAAC,EAAD,UAIJ,cAAC,EAAD,Q,2BCzCDT,EAAWlD,IAAXkD,QACA7B,EAAoBD,IAApBC,MAAOC,EAAaF,IAAbE,UA8FCsC,EA5FE,WAEb,OADAP,OAAOC,SAAS,EAAE,GAEd,eAAC,IAAD,WACA,cAAC,EAAD,CAAW1C,OAAO,MAClB,eAAC,EAAD,CAASJ,UAAU,cAAcL,MAAO,CAAEI,QAAS,SAAUgD,UAAW,IAAxE,UAEE,eAAC,IAAD,CAAYpD,MAAO,CAAEqD,OAAQ,UAA7B,UACE,cAAC,IAAW3C,KAAZ,mBACA,cAAC,IAAWA,KAAZ,uBAGF,sBAAKL,UAAU,yBAAyBL,MAAO,CAAEI,QAAS,IAA1D,UACA,cAAC,EAAD,CAAOkB,MAAO,EAAd,mBAEE,eAAC,IAAD,CAAOsB,UAAU,WAAWrB,KAAK,SAASvB,MAAO,CAAEG,MAAO,QAA1D,UAEA,cAAC,IAAD,CACIiC,QAAQ,UACRC,YAAY,yCACZC,KAAK,UACLC,UAAQ,EACRmB,UAAQ,IAGZ,cAAC,IAAD,CACItB,QAAQ,UACRC,YAAY,8IACZC,KAAK,UACLC,UAAQ,EACRmB,UAAQ,IAGZ,cAAC,IAAD,CACItB,QAAQ,cACRC,YAAY,sFACZC,KAAK,OACLC,UAAQ,EACRmB,UAAQ,IAGZ,cAAC,IAAD,CAASF,YAAY,OAArB,SAA4B,cAAC,EAAD,CAAOlC,MAAO,EAAd,8BAE5B,eAAC,IAAD,CAAMgB,KAAK,QAAQqB,MAAM,6BAA6BC,MAAO,eAAC,IAAD,CAAMjD,GAAG,gCAAT,kBAA8C,cAACkD,EAAA,EAAD,OAA3G,UACI,cAAC,IAAD,CAAKC,MAAM,OAAX,4BADJ,IAC2C,cAAC,IAAD,CAAKA,MAAM,OAAX,qCAA+C,cAAC,IAAD,CAAKA,MAAM,OAAX,8BACtF,cAAC,IAAD,IACA,cAAC,EAAD,CACAC,SAAU,CACNC,KAAM,EACNC,YAAY,GAHhB,22BAOJ,eAAC,IAAD,CAAM3B,KAAK,QAAQqB,MAAM,eAAeC,MAAO,eAAC,IAAD,CAAMjD,GAAG,oBAAT,kBAAkC,cAACkD,EAAA,EAAD,OAAjF,UACI,cAAC,IAAD,CAAKC,MAAM,OAAX,4BADJ,IAC2C,cAAC,IAAD,CAAKA,MAAM,OAAX,qCAA+C,cAAC,IAAD,CAAKA,MAAM,OAAX,8BACtF,cAAC,IAAD,IACA,cAAC,EAAD,CACAC,SAAU,CACNC,KAAM,EACNC,YAAY,GAHhB,g7CAMJ,eAAC,IAAD,CAAM3B,KAAK,QAAQqB,MAAM,4BAA4BC,MAAO,eAAC,IAAD,CAAMjD,GAAG,yBAAT,kBAAuC,cAACkD,EAAA,EAAD,OAAnG,UACI,cAAC,IAAD,CAAKC,MAAM,OAAX,4BADJ,IAC2C,cAAC,IAAD,CAAKA,MAAM,OAAX,qCAA+C,cAAC,IAAD,CAAKA,MAAM,OAAX,8BACtF,cAAC,IAAD,IACA,cAAC,EAAD,CACAC,SAAU,CACNC,KAAM,EACNC,YAAY,GAHhB,2wCAOJ,cAAC,IAAD,CAAST,YAAY,OAArB,SAA4B,cAAC,EAAD,CAAOlC,MAAO,EAAd,uCAE5B,eAAC,IAAD,CAAMgB,KAAK,QAAQqB,MAAM,wBAAwBC,MAAO,eAAC,IAAD,CAAMjD,GAAG,4BAAT,kBAA0C,cAACkD,EAAA,EAAD,OAAlG,UACI,cAAC,IAAD,CAAKC,MAAM,SAAX,oBADJ,IACqC,cAAC,IAAD,CAAKA,MAAM,OAAX,qCAA+C,cAAC,IAAD,CAAKA,MAAM,OAAX,8BAChF,cAAC,IAAD,IACA,cAAC,EAAD,CACAC,SAAU,CACNC,KAAM,EACNC,YAAY,GAHhB,upBAUR,cAAC,EAAD,Q,kCC3FDlB,EAAWlD,IAAXkD,QACA7B,GAAeD,IAAfC,MAAO8B,GAAQ/B,IAAR+B,KA+JCkB,GA5JG,WAEd,OADAhB,OAAOC,SAAS,EAAE,GAEd,eAAC,IAAD,WACA,cAAC,EAAD,CAAW1C,OAAO,MAClB,eAAC,EAAD,CAASJ,UAAU,cAAcL,MAAO,CAAEI,QAAS,SAAUgD,UAAW,IAAxE,UAEE,eAAC,IAAD,CAAYpD,MAAO,CAAEqD,OAAQ,UAA7B,UACE,cAAC,IAAW3C,KAAZ,mBACA,cAAC,IAAWA,KAAZ,uBAGF,sBAAKL,UAAU,yBAAyBL,MAAO,CAAEI,QAAS,IAA1D,UAEA,cAAC,GAAD,CAAOkB,MAAO,EAAd,gCAEE,qBAAKjB,UAAU,oBAAf,SACA,eAAC,IAAD,CACIuC,UAAU,WACV5C,MAAO,CAACG,MAAO,QAFnB,UAIA,eAAC,IAAD,CAAKgE,OAAQ,GAAb,UACA,cAAC,IAAD,CAAKC,KAAM,EAAX,SACA,mBAAGrC,KAAK,8DAAR,SACI,cAAC,IAAD,CAAM4B,MAAM,WAAWU,UAAU,EAAOC,WAAW,EAAnD,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC2B,EAAA,EAAD,CAAiBvE,MAAO,CAAEwE,SAAU,UACpC,cAAC,GAAD,CAAMlC,KAAK,YAAX,wBACA,cAAC,IAAD,CAAKwB,MAAM,QAAX,6BAKJ,cAAC,IAAD,CAAKM,KAAM,EAAX,SACA,mBAAGrC,KAAK,8DAAR,SACI,cAAC,IAAD,CAAM4B,MAAM,aAAaU,UAAU,EAAOC,WAAW,EAArD,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC2B,EAAA,EAAD,CAAiBvE,MAAO,CAAEwE,SAAU,UACpC,cAAC,GAAD,CAAMlC,KAAK,YAAX,wBACA,cAAC,IAAD,CAAKwB,MAAM,QAAX,6BAKJ,cAAC,IAAD,CAAKM,KAAM,EAAX,SACA,4BACI,cAAC,IAAD,CAAMT,MAAM,OAAOU,UAAU,EAAOC,WAAW,EAA/C,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC2B,EAAA,EAAD,CAAiBvE,MAAO,CAAEwE,SAAU,UACpC,cAAC,IAAD,CAAKV,MAAM,MAAX,6BAMJ,eAAC,IAAD,CAAKK,OAAQ,GAAb,UACA,cAAC,IAAD,CAAKC,KAAM,EAAX,SACA,mBAAGrC,KAAK,8DAAR,SACI,cAAC,IAAD,CAAM4B,MAAM,UAAUU,UAAU,EAAOC,WAAW,EAAlD,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC2B,EAAA,EAAD,CAAiBvE,MAAO,CAAEwE,SAAU,UACpC,cAAC,GAAD,CAAMlC,KAAK,YAAX,wBACA,cAAC,IAAD,CAAKwB,MAAM,QAAX,6BAKJ,cAAC,IAAD,CAAKM,KAAM,EAAX,SACA,mBAAGrC,KAAK,8DAAR,SACI,cAAC,IAAD,CAAM4B,MAAM,YAAYU,UAAU,EAAOC,WAAW,EAApD,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC2B,EAAA,EAAD,CAAiBvE,MAAO,CAAEwE,SAAU,UACpC,cAAC,GAAD,CAAMlC,KAAK,YAAX,wBACA,cAAC,IAAD,CAAKwB,MAAM,QAAX,6BAKJ,cAAC,IAAD,CAAKM,KAAM,EAAX,SACA,mBAAGrC,KAAK,8DAAR,SACI,cAAC,IAAD,CAAM4B,MAAM,cAAcU,UAAU,EAAOC,WAAW,EAAtD,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC2B,EAAA,EAAD,CAAiBvE,MAAO,CAAEwE,SAAU,UACpC,cAAC,GAAD,CAAMlC,KAAK,YAAX,uBACA,cAAC,IAAD,CAAKwB,MAAM,QAAX,gCAMJ,eAAC,IAAD,CAAKK,OAAQ,GAAb,UACA,cAAC,IAAD,CAAKC,KAAM,EAAX,SACA,4BACI,cAAC,IAAD,CAAMT,MAAM,YAAYU,UAAU,EAAOC,WAAW,EAApD,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC2B,EAAA,EAAD,CAAiBvE,MAAO,CAAEwE,SAAU,UACpC,cAAC,IAAD,CAAKV,MAAM,MAAX,0BAKJ,cAAC,IAAD,CAAKM,KAAM,EAAX,SACA,4BACI,cAAC,IAAD,CAAMT,MAAM,iBAAiBU,UAAU,EAAOC,WAAW,EAAzD,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC2B,EAAA,EAAD,CAAiBvE,MAAO,CAAEwE,SAAU,UACpC,cAAC,IAAD,CAAKV,MAAM,MAAX,0BAKJ,cAAC,IAAD,CAAKM,KAAM,EAAX,SACA,4BACI,cAAC,IAAD,CAAMT,MAAM,iBAAiBU,UAAU,EAAOC,WAAW,EAAzD,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC2B,EAAA,EAAD,CAAiBvE,MAAO,CAAEwE,SAAU,UACpC,cAAC,IAAD,CAAKV,MAAM,MAAX,6BAMJ,eAAC,IAAD,CAAKK,OAAQ,GAAb,UACA,cAAC,IAAD,CAAKC,KAAM,EAAX,SACA,mBAAGrC,KAAK,4IAAR,SACI,cAAC,IAAD,CAAM4B,MAAM,WAAWU,UAAU,EAAOC,WAAW,EAAnD,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC6B,EAAA,EAAD,CAAczE,MAAO,CAAEwE,SAAU,UACjC,cAAC,GAAD,CAAMlC,KAAK,YAAX,4BACA,cAAC,IAAD,CAAKwB,MAAM,QAAX,6BAKJ,cAAC,IAAD,CAAKM,KAAM,EAAX,SACA,mBAAGrC,KAAK,6IAAR,SACI,cAAC,IAAD,CAAM4B,MAAM,WAAWU,UAAU,EAAOC,WAAW,EAAnD,SACA,eAAC,IAAD,CAAO1B,UAAU,WAAjB,UACA,cAAC6B,EAAA,EAAD,CAAczE,MAAO,CAAEwE,SAAU,UACjC,cAAC,GAAD,CAAMlC,KAAK,YAAX,4BACA,cAAC,IAAD,CAAKwB,MAAM,QAAX,2CAWR,cAAC,EAAD,Q,8CCpJOY,OAXf,SAAuB3E,GACnB,OACI,cAAC,KAAD,CACIM,UAAU,mBACVsE,OAAQ,kBAAMzB,OAAO0B,QAAQC,QAC7BlB,MAAO5D,EAAM4D,MACbmB,SAAU/E,EAAM+E,YCKpB5D,GAA2BD,IAA3BC,MAAO8B,GAAoB/B,IAApB+B,KAAM7B,GAAcF,IAAdE,UACb4B,GAAYlD,IAAZkD,QACFgC,GAAYpD,YAkBHqD,OAhBf,SAAgCjF,GAE5B,OADAmD,OAAOC,SAAS,EAAE,GAEtB,eAAC,IAAD,WACQ,cAAC,EAAD,CAAW1C,OAAO,MAClB,eAAC,GAAD,CAASJ,UAAU,cAAcL,MAAO,CAAEI,QAAS,SAAUgD,UAAW,IAAxE,UACA,cAAC,GAAD,CAAeO,MAAM,+BACnB,qBAAKtD,UAAU,yBAAyBL,MAAO,CAAEI,QAAS,IAA1D,SACI,cAAC6E,GAAD,SAGN,cAAC,EAAD,QAOR,SAASA,KACL,OACI,eAAC,IAAD,CAAQjF,MAAO,CAAEqB,gBAAiB,QAASjB,QAAS,KAApD,UACA,gCACA,cAAC,IAAD,CAAK0D,MAAM,OAAX,4BADA,IACuC,cAAC,IAAD,CAAKA,MAAM,OAAX,qCAA+C,cAAC,IAAD,CAAKA,MAAM,OAAX,iCAEtF,cAAC,IAAD,IACA,cAAC,GAAD,y2BAEA,eAAC,GAAD,+MAA4C,cAAC,GAAD,CAAMR,QAAM,EAAZ,kDAA5C,8BAEA,cAAC,GAAD,CAAOhC,MAAO,EAAd,2DACA,cAAC,GAAD,+JACA,cAAC,aAAD,CAAW4D,KAAK,mDAChB,eAAC,GAAD,4ZAA+E,cAAC,cAAD,CAAYA,KAAK,SAAhG,OACA,eAAC,GAAD,sFAAuB,cAAC,cAAD,CAAYA,KAAK,6CACxC,eAAC,GAAD,CAAW5C,KAAK,YAAhB,oEAAqD,cAAC,cAAD,CAAY4C,KAAK,kEAAtE,2CACA,eAAC,GAAD,oHAA4B,cAAC,GAAD,CAAM5B,QAAM,EAAZ,kJAA5B,uFAAoF,cAAC,cAAD,CAAY4B,KAAK,SAArG,8EAAsI,cAAC,cAAD,CAAYA,KAAK,SAAvJ,qHAAkL,cAAC,cAAD,CAAYA,KAAK,SAAnM,iIAAgO,cAAC,cAAD,CAAYA,KAAK,SAAjP,4EAAsQ,cAAC,cAAD,CAAYA,KAAK,SAAvR,gDAEA,cAAC,GAAD,CAAO5D,MAAO,EAAd,2DACA,cAAC,GAAD,2QACA,eAAC,GAAD,kMAAyC,cAAC,cAAD,CAAY4D,KAAK,YAA1D,qGAAsF,cAAC,cAAD,CAAYA,KAAK,yBAAvG,2EAA0I,cAAC,cAAD,CAAYA,KAAK,2FAA3J,sGACA,iCAAQ,cAAC,IAAD,CACJ/E,MAAM,QACNuB,IAAG,UAAKqD,GAAL,+BACHnD,SAAUuD,MAEd,cAAC,aAAD,CAAWD,KAAK,yHAEhB,cAAC,GAAD,CAAO5D,MAAO,EAAd,6EACA,eAAC,GAAD,wSAA0D,cAAC,GAAD,CAAMgC,QAAM,EAAZ,4FAA1D,qEAAsG,cAAC,GAAD,CAAMhB,KAAK,UAAX,wDAAtG,sGACA,cAAC,GAAD,uPACA,iCAAQ,eAAC,IAAD,WACR,cAAC,IAAD,CACInC,MAAM,QACNuB,IAAG,UAAKqD,GAAL,+BACHnD,SAAUuD,IAEd,cAAC,IAAD,CACIhF,MAAM,QACNuB,IAAG,UAAKqD,GAAL,+BACHnD,SAAUuD,SAGd,cAAC,KAAD,CAAmBC,SAAS,SAASpF,MAAOqF,KAC5CC,SAAQ,mtBAmCR,eAAC,GAAD,kGAAyB,cAAC,cAAD,CAAYJ,KAAK,4BAA1C,+YAAsI,cAAC,cAAD,CAAYA,KAAK,4BAAvJ,wBACA,iCAAQ,cAAC,IAAD,CACJ/E,MAAM,QACNuB,IAAG,UAAKqD,GAAL,+BACHnD,SAAUuD,MAEd,cAAC,KAAD,CACAnF,MAAOqF,KACPD,SAAS,SACTE,SAAQ,wgBAsBR,cAAC,GAAD,CAAOhE,MAAO,EAAd,6EACA,eAAC,GAAD,q5BAA8K,cAAC,cAAD,CAAY4D,KAAK,SAA/L,iCAA4M,cAAC,cAAD,CAAYA,KAAK,SAA7N,kBACA,eAAC,GAAD,+RAA2D,cAAC,cAAD,CAAYA,KAAK,SAA5E,SAAqF,cAAC,cAAD,CAAYA,KAAK,SAAtG,kKAA2I,cAAC,cAAD,CAAYA,KAAK,YAA5J,sIAAgM,cAAC,cAAD,CAAYA,KAAK,YAAjN,6CAAkO,cAAC,cAAD,CAAYA,KAAK,SAAnP,2BAA+P,cAAC,cAAD,CAAYA,KAAK,SAAhR,uCAA8R,cAAC,cAAD,CAAYA,KAAK,YAA/S,qCACA,cAAC,GAAD,CAAO5D,MAAO,EAAd,sCACA,eAAC,GAAD,iOAA+C,cAAC,cAAD,CAAY4D,KAAK,SAAhE,qBAA2E,cAAC,cAAD,CAAYA,KAAK,SAA5F,6CAA2G,cAAC,cAAD,CAAYA,KAAK,mBAA5H,qBAAgJ,cAAC,cAAD,CAAYA,KAAK,aAAjK,sEAAkM,cAAC,cAAD,CAAYA,KAAK,MAAnN,8KAEA,cAAC,GAAD,uSAGA,eAAC,GAAD,0EAAqB,cAAC,cAAD,CAAYA,KAAK,yCAEtC,cAAC,GAAD,CAAO5D,MAAO,EAAd,oEACA,eAAC,GAAD,4JAAsC,cAAC,cAAD,CAAY4D,KAAK,YAAvD,scAA8I,cAAC,cAAD,CAAYA,KAAK,YAA/J,6GAEA,eAAC,GAAD,8JAAmC,cAAC,cAAD,CAAYA,KAAK,SAApD,2JAAuF,cAAC,cAAD,CAAYA,KAAK,UAAxG,6FAAgI,cAAC,cAAD,CAAYA,KAAK,MAAjJ,2BAA0J,cAAC,cAAD,CAAYA,KAAK,SAA3K,2EAA+L,cAAC,cAAD,CAAYA,KAAK,SAAhN,wHACA,cAAC,aAAD,CAAWA,KAAK,4IAChB,eAAC,GAAD,gCAAc,cAAC,cAAD,CAAYA,KAAK,UAA/B,6IAA8D,cAAC,cAAD,CAAYA,KAAK,MAA/E,qBAAuF,cAAC,cAAD,CAAYA,KAAK,MAAxG,6DACA,eAAC,GAAD,gIAA8B,cAAC,cAAD,CAAYA,KAAK,YAA/C,4KAAsF,cAAC,cAAD,CAAYA,KAAK,+CAAvG,sBAAqJ,cAAC,cAAD,CAAYA,KAAK,YAAtK,4DACA,cAAC,aAAD,CAAWA,KAAK,yLAChB,eAAC,GAAD,CAAW5C,KAAK,YAAhB,0EAAuC,cAAC,cAAD,CAAY4C,KAAK,mBAAxD,0DAAmF,cAAC,cAAD,CAAYA,KAAK,YAApG,wCAAqH,cAAC,cAAD,CAAYA,KAAK,wDAAtI,mBAEA,eAAC,GAAD,CAAW5C,KAAK,YAAhB,mHAA6C,cAAC,cAAD,CAAY4C,KAAK,aAA9D,+GAA2F,cAAC,cAAD,CAAYA,KAAK,kBAA5G,yDAAqI,cAAC,GAAD,CAAM5B,QAAM,EAAZ,sCAArI,eAA+J,cAAC,cAAD,CAAY4B,KAAK,uDAChL,cAAC,aAAD,CAAWA,KAAK,mKADhB,2JAIA,cAAC,GAAD,ghB,ICjKAhE,GAAiCD,IAAjCC,MAAO8B,GAA0B/B,IAA1B+B,KAAM7B,GAAoBF,IAApBE,UAAWH,GAASC,IAATD,KACxB+B,GAAYlD,IAAZkD,QACFgC,GAAYpD,YAkBH4D,OAhBf,WAEI,OADArC,OAAOC,SAAS,EAAE,GAEd,eAAC,IAAD,WACA,cAAC,EAAD,CAAW1C,OAAO,MAClB,eAAC,GAAD,CAASJ,UAAU,cAAcL,MAAO,CAAEI,QAAS,SAAUgD,UAAW,IAAxE,UACA,cAAC,GAAD,CAAeO,MAAM,iBACnB,qBAAKtD,UAAU,yBAAyBL,MAAO,CAAEI,QAAS,IAA1D,SACI,cAAC,GAAD,SAGN,cAAC,EAAD,QAOR,SAAS6E,KACL,OACA,eAAC,IAAD,CAAQjF,MAAO,CAAEqB,gBAAiB,QAASjB,QAAS,KAApD,UACI,gCACA,cAAC,IAAD,CAAK0D,MAAM,OAAX,4BADA,IACuC,cAAC,IAAD,CAAKA,MAAM,OAAX,qCADvC,IACuF,cAAC,IAAD,CAAKA,MAAM,OAAX,8BACvF,cAAC,IAAD,OAEA,cAAC,GAAD,86CACA,cAAC,GAAD,CAAOxC,MAAO,EAAd,2EACA,cAAC,GAAD,UAAW,cAAC,GAAD,CAAMgC,QAAM,EAAZ,yCACX,cAAC,GAAD,UAAW,cAAC,GAAD,CAAMA,QAAM,EAAZ,qDACX,cAAC,GAAD,UAAW,cAAC,GAAD,CAAMA,QAAM,EAAZ,qDACX,cAAC,GAAD,UAAW,cAAC,GAAD,CAAMA,QAAM,EAAZ,qDACX,cAAC,GAAD,UAAW,cAAC,GAAD,CAAMA,QAAM,EAAZ,yCACX,cAAC,GAAD,UAAW,cAAC,GAAD,CAAMA,QAAM,EAAZ,2CACX,uBACA,cAAC,GAAD,CAAOhC,MAAO,EAAd,uCACA,eAAC,GAAD,iIAAwD,cAAC,GAAD,CAAMgC,QAAM,EAAZ,kDAAxD,2WAC0D,cAAC,cAAD,CAAY4B,KAAK,MAD3E,mDACwF,cAAC,cAAD,CAAYA,KAAK,UADzG,6CACyH,cAAC,GAAD,CAAM5B,QAAM,EAAZ,sCADzH,4NAE+B,cAAC,GAAD,CAAMA,QAAM,EAAZ,kDAF/B,yGAEwG,cAAC,GAAD,CAAMA,QAAM,EAAZ,sCAFxG,sEAE4I,cAAC,cAAD,CAAY4B,KAAK,MAF7J,4DAGS,cAAC,cAAD,CAAYA,KAAK,QAH1B,kOAGuE,cAAC,GAAD,CAAM5B,QAAM,EAAZ,kDAHvE,qKAIA,iCAAQ,cAAC,IAAD,CAAOkC,IAAI,0BAA0B9D,IAAG,UAAKqD,GAAL,aAA2B5E,MAAM,QAAQyB,SAAUuD,MACnG,eAAC,GAAD,sCAAe,cAAC,cAAD,CAAYD,KAAK,QAAhC,eAAyC,cAAC,cAAD,CAAYA,KAAK,MAA1D,mDAAuE,cAAC,cAAD,CAAYA,KAAK,QAAxF,eAAiG,cAAC,cAAD,CAAYA,KAAK,MAAlH,yGAAwI,cAAC,cAAD,CAAYA,KAAK,MAAzJ,qEAAyK,cAAC,cAAD,CAAYA,KAAK,SAA1L,mDAA0M,cAAC,cAAD,CAAYA,KAAK,MAA3N,wBACA,eAAC,GAAD,kJAAiC,cAAC,GAAD,CAAM5B,QAAM,EAAZ,4CAAjC,yNAEA,eAAC,GAAD,kJAAiC,cAAC,GAAD,CAAMA,QAAM,EAAZ,kDAAjC,mKAEA,cAAC,GAAD,CAAWhB,KAAK,YAAhB,SACA,eAAC,GAAD,mQAAiF,cAAC,GAAD,CAAMgB,QAAM,EAAZ,kBAAjF,eAEA,eAAC,GAAD,oLAA0C,cAAC,cAAD,CAAY4B,KAAK,UAA3D,kEAEA,cAAC,GAAD,CAAW5C,KAAK,YAAhB,SACA,cAAC,GAAD,+QAEA,cAAC,GAAD,myBAGA,eAAC,GAAD,2MAA6C,cAAC,GAAD,CAAMgB,QAAM,EAAZ,kDAA7C,2MAC+B,cAAC,GAAD,CAAMA,QAAM,EAAZ,kDAD/B,2SAEuC,cAAC,cAAD,CAAY4B,KAAK,iBAFxD,SAEyE,cAAC,cAAD,CAAYA,KAAK,iBAF1F,iLAEuI,cAAC,GAAD,CAAM5B,QAAM,EAAZ,qHAFvI,wHAIA,cAAC,GAAD,+tCAIA,cAAC,GAAD,CAAOhC,MAAO,EAAd,mDACA,eAAC,GAAD,gMAA4C,cAAC,GAAD,CAAMgC,QAAM,EAAZ,kDAA5C,yZAEA,cAAC,GAAD,CAAOhC,MAAO,EAAd,oDACA,cAAC,GAAD,8WAEA,cAAC,GAAD,CAAWgB,KAAK,YAAhB,SACA,cAAC,GAAD,wiBAGA,cAAC,GAAD,sfAEA,+BACA,8RACA,qRAEA,cAAC,GAAD,wVAEA,cAAC,GAAD,CAAOhB,MAAO,EAAd,mDACA,cAAC,GAAD,kIACA,iCAAQ,cAAC,IAAD,CAAOkE,IAAI,0BAA0B9D,IAAG,UAAKqD,GAAL,aAA2B5E,MAAM,MAAMyB,SAAUuD,MACjG,eAAC,GAAD,iFAAuB,cAAC,cAAD,CAAYD,KAAK,QAAxC,uCAAqD,cAAC,cAAD,CAAYA,KAAK,QAAtE,iCAAkF,cAAC,cAAD,CAAYA,KAAK,MAAnG,2BAA4G,cAAC,cAAD,CAAYA,KAAK,MAA7H,iCAAuI,cAAC,cAAD,CAAYA,KAAK,QAAxJ,kMACc,cAAC,cAAD,CAAYA,KAAK,QAD/B,8BAEA,cAAC,GAAD,0MACA,iCAAQ,cAAC,IAAD,CAAOM,IAAI,0BAA0B9D,IAAG,UAAKqD,GAAL,aAA2B5E,MAAM,MAAMyB,SAAUuD,MACjG,cAAC,GAAD,0aAEA,eAAC,GAAD,CAAW7C,KAAK,YAAhB,UACA,cAAC,GAAD,kEACA,cAAC,GAAD,qMACA,cAAC,GAAD,uCACA,eAAC,GAAD,6MAAyD,cAAC,cAAD,CAAY4C,KAAK,MAA1E,yCACI,cAAC,cAAD,CAAYA,KAAK,MADrB,gIACmD,cAAC,cAAD,CAAYA,KAAK,SADpE,UAEA,cAAC,aAAD,CAAWA,KAAK,6CAFhB,gHAKmB,cAAC,cAAD,CAAYA,KAAK,SALpC,wFAKqE,cAAC,cAAD,CAAYA,KAAK,YALtF,sBAKoG,cAAC,cAAD,CAAYA,KAAK,kBAErH,cAAC,GAAD,CAAO5D,MAAO,EAAd,kEACA,eAAC,GAAD,mdAC0B,cAAC,cAAD,CAAY4D,KAAK,QAD3C,uCACwD,cAAC,cAAD,CAAYA,KAAK,MADzE,gHAEe,cAAC,cAAD,CAAYA,KAAK,QAFhC,0IAGA,iCAAQ,cAAC,IAAD,CAAOM,IAAI,0BAA0B9D,IAAG,UAAKqD,GAAL,aAA2B5E,MAAM,MAAMyB,SAAUuD,MACjG,cAAC,GAAD,iJACA,cAAC,GAAD,CAAO7D,MAAO,EAAd,uCACA,iCAAQ,cAAC,IAAD,CAAOkE,IAAI,0BAA0B9D,IAAG,UAAKqD,GAAL,aAA2B5E,MAAM,MAAMyB,SAAUuD,MACjG,cAAC,GAAD,0lBAEA,eAAC,GAAD,8mBAC+D,cAAC,cAAD,CAAYD,KAAK,WADhF,w1BAEqH,cAAC,GAAD,CAAM5B,QAAM,EAAZ,+DAFrH,4GAIA,eAAC,GAAD,sIAA+B,cAAC,cAAD,CAAY4B,KAAK,QAAhD,4EACA,cAAC,aAAD,CAAWA,KAAK,2DAGhB,cAAC,GAAD,CAAO5D,MAAO,EAAd,2CACA,eAAC,GAAD,sFAAuB,cAAC,cAAD,CAAY4D,KAAK,MAAxC,ilBAGA,iCAAQ,cAAC,IAAD,CAAOM,IAAI,0BAA0B9D,IAAG,UAAKqD,GAAL,aAA2B5E,MAAM,MAAMyB,SAAUuD,MACjG,+BACA,0GACA,8SAEA,cAAC,GAAD,uJACA,+BACA,6BACA,cAAC,GAAD,UAAW,cAAC,aAAD,CAAWD,KAAK,6DAI3B,6BACA,cAAC,GAAD,UAAW,cAAC,aAAD,CAAWA,KAAK,+DAK3B,cAAC,GAAD,CAAO5D,MAAO,EAAd,uCACA,eAAC,GAAD,6YACc,cAAC,cAAD,CAAY4D,KAAK,oBAD/B,iOAEkB,cAAC,cAAD,CAAYA,KAAK,YAFnC,aAGA,eAAC,GAAD,CAAW5C,KAAK,YAAhB,UACA,cAAC,GAAD,UAAW,cAAC,GAAD,CAAMgB,QAAM,EAAZ,8HACX,cAAC,GAAD,oKACA,cAAC,GAAD,CAAMvB,KAAK,2FAAX,sGACA,cAAC,GAAD,qJACA,+BACA,0DAAQ,cAAC,GAAD,CAAMuB,QAAM,EAAZ,kDAAR,4MAEA,sHAAkB,cAAC,GAAD,CAAMA,QAAM,EAAZ,yCAClB,4FACA,gIAGA,eAAC,GAAD,6QAAkE,cAAC,cAAD,CAAY4B,KAAK,MAAnF,gDACA,uBACA,cAAC,GAAD,CAAO5D,MAAO,EAAd,wCACA,cAAC,GAAD,6QACA,iCAAQ,cAAC,IAAD,CAAOkE,IAAI,0BAA0B9D,IAAG,UAAKqD,GAAL,aAA2B5E,MAAM,MAAMyB,SAAUuD,MACjG,cAAC,GAAD,0lBAEA,eAAC,GAAD,4GAA8B,cAAC,GAAD,CAAM7B,QAAM,EAAZ,iIAA9B,uJAEA,cAAC,GAAD,CAAOhC,MAAO,EAAd,yCACA,eAAC,GAAD,2DAA8C,6EAA9C,2DACA,eAAC,GAAD,sDAAyC,2CAAzC,wFACA,eAAC,GAAD,0IAAwC,2LAAxC,yDACA,eAAC,GAAD,4FAA6B,wJAA7B,2DACA,eAAC,GAAD,0KAA0C,sOAA1C,2DAEA,eAAC,GAAD,mFAAsE,gDAAtE,0G,ICvL6DJ,GAA2BD,IAA3BC,MAAO8B,GAAoB/B,IAApB+B,KAAM7B,GAAcF,IAAdE,UAC1E4B,GAAYlD,IAAZkD,QACFgC,GAAYpD,YAgBH8D,OAff,WAEG,OADAvC,OAAOC,SAAS,EAAE,GAEd,eAAC,IAAD,WACI,cAAC,EAAD,CAAW1C,OAAO,MAClB,eAAC,GAAD,CAASJ,UAAU,cAAcL,MAAO,CAAEI,QAAS,SAAUgD,UAAW,IAAxE,UACA,cAAC,GAAD,CAAeO,MAAM,qBACrB,qBAAKtD,UAAU,yBAAyBL,MAAO,CAAEI,QAAS,IAA1D,SACG,cAAC,GAAD,SAGH,cAAC,EAAD,QAKX,SAAS6E,KACR,OAAO,eAAC,IAAD,CAAQjF,MAAO,CAAEqB,gBAAiB,QAASjB,QAAS,KAApD,UACJ,gCACI,cAAC,IAAD,CAAK0D,MAAM,OAAX,4BADJ,IAC2C,cAAC,IAAD,CAAKA,MAAM,OAAX,qCAA+C,cAAC,IAAD,CAAKA,MAAM,OAAX,iCAE1F,cAAC,IAAD,IACJ,cAAC,GAAD,ywCAGA,cAAC,GAAD,CAAOxC,MAAO,EAAd,sEACA,cAAC,GAAD,UAAW,mBAAGS,KAAK,uCAAR,qFACX,cAAC,IAAD,IACA,cAAC,GAAD,UAAW,cAAC,GAAD,CAAMuB,QAAM,EAAZ,4BACX,+BACA,wFACA,gHACA,oGACA,wFACA,yHAEA,cAAC,IAAD,IACA,cAAC,GAAD,CAAOhC,MAAO,EAAd,uEACA,cAAC,GAAD,ohBACA,cAAC,GAAD,uRACA,cAAC,GAAD,UAAW,cAAC,IAAD,CAAOkE,IAAI,uFAAiB9D,IAAG,UAAKqD,GAAL,eAA4B5E,MAAM,MAAMH,MAAO,CAAC0F,SAAS,SAAU9D,SAAUuD,MACvH,cAAC,GAAD,uoBACA,cAAC,GAAD,iLACA,cAAC,GAAD,CAAO7D,MAAO,EAAd,+FACA,cAAC,GAAD,y5CACA,cAAC,GAAD,UAAW,cAAC,IAAD,CAAOkE,IAAI,uFAAiB9D,IAAG,UAAKqD,GAAL,eAA6B5E,MAAM,MAAMH,MAAO,CAAC0F,SAAS,SAAU9D,SAAUuD,MACxH,cAAC,GAAD,mYACA,cAAC,GAAD,2cACA,cAAC,GAAD,CAAO7D,MAAO,EAAd,mFACA,eAAC,GAAD,+rCAAoN,cAAC,cAAD,CAAY4D,KAAK,SAArO,6BAAmP,cAAC,cAAD,CAAYA,KAAK,MAApQ,oOAAiT,cAAC,cAAD,CAAYA,KAAK,aAAlU,mTACA,cAAC,GAAD,mpCACA,cAAC,GAAD,UAAW,mBAAGnD,KAAK,wDAAwD4D,MAAM,WAAtE,oDACX,cAAC,GAAD,CAAOrE,MAAO,EAAd,uEACA,eAAC,GAAD,uSAAuF,cAAC,cAAD,CAAY4D,KAAK,cAAxG,qRAA8L,cAAC,cAAD,CAAYA,KAAK,cAA/M,oDAAoO,cAAC,cAAD,CAAYA,KAAK,cAArP,uKACA,cAAC,GAAD,UAAW,cAAC,IAAD,CAAOM,IAAI,4BAA4B9D,IAAG,UAAKqD,GAAL,eAA6B5E,MAAM,MAAMH,MAAO,CAAC0F,SAAS,SAAU9D,SAAUuD,MACnI,cAAC,GAAD,uvBACA,cAAC,GAAD,UAAW,cAAC,IAAD,CAAOK,IAAI,2IAAuC9D,IAAG,UAAKqD,GAAL,eAA6B5E,MAAM,MAAMH,MAAO,CAAC0F,SAAS,SAAU9D,SAAUuD,MAC9I,cAAC,GAAD,u+BACA,cAAC,GAAD,CAAO7D,MAAO,EAAd,+FACA,cAAC,GAAD,CAAOA,MAAO,EAAd,0BACA,cAAC,GAAD,ugBACA,cAAC,GAAD,UAAW,cAAC,IAAD,CAAOkE,IAAI,yBAAyB9D,IAAG,UAAKqD,GAAL,eAA6B5E,MAAM,MAAMH,MAAO,CAAC0F,SAAS,SAAU9D,SAAUuD,MAChI,cAAC,GAAD,+eACA,cAAC,GAAD,CAAO7D,MAAO,EAAd,uBACA,cAAC,GAAD,UAAW,mBAAGS,KAAK,uEAAuE4D,MAAM,WAArF,sEACX,cAAC,GAAD,8qC,IClEqEzE,GAA2BD,IAA3BC,MAAO8B,GAAoB/B,IAApB+B,KAAM7B,GAAcF,IAAdE,UAC1E4B,GAAYlD,IAAZkD,QAgBO6C,OAff,WAEG,OADA1C,OAAOC,SAAS,EAAE,GAEd,eAAC,IAAD,WACI,cAAC,EAAD,CAAW1C,OAAO,MAClB,eAAC,GAAD,CAASJ,UAAU,cAAcL,MAAO,CAAEI,QAAS,SAAUgD,UAAW,IAAxE,UACA,cAAC,GAAD,CAAeO,MAAM,0BACrB,qBAAKtD,UAAU,yBAAyBL,MAAO,CAAEI,QAAS,IAA1D,SACG,cAAC,GAAD,SAGH,cAAC,EAAD,QAKX,SAAS6E,KACR,OACD,eAAC,IAAD,CAAQjF,MAAO,CAAEqB,gBAAiB,QAASjB,QAAS,KAApD,UACA,gCACQ,cAAC,IAAD,CAAK0D,MAAM,OAAX,qCAA+C,cAAC,IAAD,CAAKA,MAAM,OAAX,8BAAwC,cAAC,IAAD,CAAKA,MAAM,SAAX,uBAE/F,cAAC,IAAD,IACA,cAAC,GAAD,4oBACA,cAAC,GAAD,CAAOxC,MAAO,EAAd,8FACA,cAAC,GAAD,UAAW,cAAC,GAAD,CAAMgC,QAAM,EAAZ,4BACX,cAAC,GAAD,CAAWhB,KAAK,YAAhB,SACA,+BACA,gHACA,kFACA,0GACA,0GACA,4HACA,+DAGA,cAAC,GAAD,CAAOhB,MAAO,EAAd,8FACA,eAAC,GAAD,wGAA0B,cAAC,GAAD,CAAMgC,QAAM,EAAZ,sCAA1B,SAAmD,cAAC,GAAD,CAAMA,QAAM,EAAZ,4CAAnD,wKACA,eAAC,GAAD,WAAW,cAAC,GAAD,CAAMA,QAAM,EAAZ,sCAAX,qEAA8C,cAAC,GAAD,CAAMA,QAAM,EAAZ,sCAA9C,sPAAiH,cAAC,GAAD,CAAMA,QAAM,EAAZ,gFAAjH,0XACA,eAAC,GAAD,WAAW,cAAC,GAAD,CAAMA,QAAM,EAAZ,4CAAX,iCAAyC,cAAC,GAAD,CAAMA,QAAM,EAAZ,gFAAzC,8pBACA,cAAC,GAAD,8yBACA,eAAC,GAAD,WAAW,cAAC,GAAD,CAAMA,QAAM,EAAZ,4CAAX,0TAAiG,cAAC,GAAD,CAAMA,QAAM,EAAZ,sCAAjG,mIACA,cAAC,aAAD,CAAW4B,KAAK,wDADhB,+pBAI2H,cAAC,cAAD,CAAYA,KAAK,WAJ5I,sHAKA,cAAC,GAAD,CAAO5D,MAAO,EAAd,gEACA,eAAC,GAAD,0EAAqB,cAAC,GAAD,CAAMgC,QAAM,EAAZ,sIAArB,qHAAgF,cAAC,GAAD,CAAMA,QAAM,EAAZ,8GAAhF,IAAqH,cAAC,cAAD,CAAY4B,KAAK,8BAAtI,kHACA,cAAC,GAAD,CAAW5C,KAAK,YAAhB,SACA,cAAC,GAAD,sbAEA,eAAC,GAAD,0EAAqB,cAAC,cAAD,CAAY4C,KAAK,MAAtC,2BAA+C,cAAC,cAAD,CAAYA,KAAK,eAAhE,SAA+E,cAAC,cAAD,CAAYA,KAAK,MAAhG,6CAA4G,cAAC,cAAD,CAAYA,KAAK,yBAA7H,sBAAwJ,cAAC,cAAD,CAAYA,KAAK,MAAzK,mDAAsL,cAAC,cAAD,CAAYA,KAAK,MAAvM,2EAAwN,cAAC,cAAD,CAAYA,KAAK,mCAAzO,mDACA,eAAC,GAAD,CAAW5C,KAAK,YAAhB,UACA,eAAC,GAAD,gCAAc,cAAC,IAAD,CAAOZ,IAAI,kEAAkE8D,IAAI,0BAA0BrF,MAAM,QAAQyB,SAAUuD,OACjJ,eAAC,GAAD,qHAA6B,cAAC,cAAD,CAAYD,KAAK,oBAA9C,4EAA8E,cAAC,cAAD,CAAYA,KAAK,+BAC/F,cAAC,GAAD,mNAEA,cAAC,GAAD,UAAW,cAAC,aAAD,CAAWA,KAAK,6CAG3B,cAAC,GAAD,CAAW5C,KAAK,YAAhB,SACA,cAAC,GAAD,yCAEA,cAAC,GAAD,UAAW,cAAC,aAAD,CAAW4C,KAAK,iDAG3B,cAAC,GAAD,2ZACA,cAAC,GAAD,CAAO5D,MAAO,EAAd,yFACA,eAAC,GAAD,0QAAqD,cAAC,cAAD,CAAY4D,KAAK,QAAtE,SAA8E,cAAC,cAAD,CAAYA,KAAK,QAA/F,yTAA6J,cAAC,cAAD,CAAYA,KAAK,8BAA9K,gDACA,eAAC,GAAD,4OAAgD,cAAC,cAAD,CAAYA,KAAK,QAAjE,iRAAqH,cAAC,cAAD,CAAYA,KAAK,YAAtI,kBACA,cAAC,GAAD,UAAW,cAAC,IAAD,CAAOxD,IAAI,kEAAkE8D,IAAI,0BAA0BrF,MAAM,QAAQyB,SAAUuD,MAC9I,eAAC,GAAD,yYACA,cAAC,aAAD,CAAWD,KAAK,mGADhB,uIAKA,cAAC,aAAD,CAAWA,KAAK,qDALhB,wCAQO,cAAC,cAAD,CAAYA,KAAK,uEARxB,mGAQ2G,cAAC,cAAD,CAAYA,KAAK,kBAR5H,wBASA,eAAC,GAAD,gFACA,cAAC,aAAD,CAAWA,KAAK,kFADhB,gJAKA,cAAC,GAAD,iRACA,eAAC,GAAD,0EAAqB,cAAC,cAAD,CAAYA,KAAK,MAAtC,2NAA+E,cAAC,cAAD,CAAYA,KAAK,YAAhG,sVACA,cAAC,GAAD,CAAO5D,MAAO,EAAd,yFACA,eAAC,GAAD,4aAAgF,eAAC,GAAD,CAAMgC,QAAM,EAAZ,uDAAoB,cAAC,cAAD,CAAY4B,KAAK,MAArC,8BAAhF,2HAAyJ,cAAC,cAAD,CAAYA,KAAK,YAA1K,+GAAuM,cAAC,GAAD,CAAM5B,QAAM,EAAZ,gCAAvM,qKAAyP,cAAC,cAAD,CAAY4B,KAAK,YAA1Q,kEACA,cAAC,GAAD,6dACA,cAAC,GAAD,CAAO5D,MAAO,EAAd,qGACA,eAAC,GAAD,4OAAgD,cAAC,GAAD,CAAMgC,QAAM,EAAZ,gCAAhD,iKACA,cAAC,GAAD,CAAOhC,MAAO,EAAd,2DACA,eAAC,GAAD,qeAAqH,cAAC,cAAD,CAAY4D,KAAK,eAAtI,qHAAuK,cAAC,cAAD,CAAYA,KAAK,aAAxL,oCACA,cAAC,GAAD,UAAW,cAAC,IAAD,CAAOM,IAAI,uEAAqB9D,IAAI,kEAAkEvB,MAAM,QAAQyB,SAAUuD,MACzI,cAAC,GAAD,yMACA,cAAC,GAAD,CAAO7D,MAAO,EAAd,4GACA,cAAC,GAAD,43BACA,cAAC,GAAD,UAAW,cAAC,IAAD,CAAOI,IAAI,kEAAkEvB,MAAM,QAAQyB,SAAUuD,MAChH,cAAC,GAAD,CAAO7D,MAAO,EAAd,kEACA,eAAC,GAAD,0QAA0D,cAAC,cAAD,CAAY4D,KAAK,UAA3E,uIACA,cAAC,aAAD,CAAWA,KAAK,6HAGhB,cAAC,GAAD,CAAW5C,KAAK,YAAhB,SACA,eAAC,GAAD,oEAAoB,cAAC,cAAD,CAAY4C,KAAK,8BAArC,6GAA2F,cAAC,cAAD,CAAYA,KAAK,yCAA5G,gCAEA,eAAC,GAAD,0HAA6B,cAAC,cAAD,CAAYA,KAAK,yCAA9C,6CACA,cAAC,aAAD,CAAWA,KAAK,4IADhB,mMAKA,cAAC,KAAD,CAAmBE,SAAS,SAASpF,MAAOqF,KACpCC,SAAQ,wlBAkBhB,cAAC,GAAD,CAAOhE,MAAO,EAAd,sDACA,eAAC,GAAD,sIAA+B,cAAC,cAAD,CAAY4D,KAAK,aAAhD,SAA6D,cAAC,cAAD,CAAYA,KAAK,sBAA9E,qLACA,cAAC,GAAD,UAAW,cAAC,GAAD,CAAM5B,QAAM,EAAZ,4BACX,cAAC,GAAD,uOACA,eAAC,GAAD,uJAAsC,cAAC,cAAD,CAAY4B,KAAK,aAAvD,yXAAmI,cAAC,cAAD,CAAYA,KAAK,0BAApJ,YAEA,cAAC,KAAD,CAAmBE,SAAS,SAASpF,MAAOqF,KACpCC,SAAQ,otBAqBhB,cAAC,GAAD,CAAOhE,MAAO,EAAd,gDACA,eAAC,GAAD,kbAAgG,cAAC,GAAD,CAAMgC,QAAM,EAAZ,0BAAhG,YACA,cAAC,GAAD,CAAOhC,MAAO,EAAd,yCACA,eAAC,GAAD,wCAA2B,0JAA3B,+CACA,eAAC,GAAD,+CAAkC,2CAAlC,iFACA,eAAC,GAAD,sCAAmC,4EAAnC,yBCvKA,IAgBeuE,GAhBI,kBACf,cAAC,IAAD,UACI,eAAC,IAAD,WACI,cAAC,IAAD,CAAOC,OAAK,EAACC,KAAK,IAAIC,UAAW/C,IACjC,cAAC,IAAD,CAAO6C,OAAK,EAACC,KAAK,SAASC,UAAWvC,IACtC,cAAC,IAAD,CAAOqC,OAAK,EAACC,KAAK,SAASC,UAAW9B,KAEtC,cAAC,IAAD,CAAO4B,OAAK,EAACC,KAAK,gCAAgCC,UAAWhB,KAC7D,cAAC,IAAD,CAAOc,OAAK,EAACC,KAAK,oBAAoBC,UAAWT,KACjD,cAAC,IAAD,CAAOO,OAAK,EAACC,KAAK,yBAAyBC,UAAWP,KACtD,cAAC,IAAD,CAAOK,OAAK,EAACC,KAAK,4BAA4BC,UAAWJ,WCbtDK,OANf,WACE,OACE,cAAC,GAAD,KCOWC,GAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,8BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,GAAD,MAEFC,SAASC,eAAe,SAM1Bb,O","file":"static/js/main.bf4cfaa5.chunk.js","sourcesContent":["import React from 'react';\r\nimport '../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../index.css';\r\nimport { Layout, Menu } from 'antd';\r\nimport { Link } from 'react-router-dom';\r\n\r\nconst { Header } = Layout;\r\n\r\nfunction AppHeader(props){\r\n    return (\r\n        <Header style={{ position: 'fixed', zIndex: 1, width: '100%', padding:'0'}}>\r\n          <div className=\"logo\" />\r\n          <Menu theme=\"light\" mode=\"horizontal\" defaultSelectedKeys={[props.select]}>\r\n            <Menu.Item key=\"1\">\r\n              <Link to=\"/\">Home</Link>\r\n            </Menu.Item>\r\n            <Menu.Item key=\"2\">\r\n              <Link to=\"/posts\">Posts</Link>\r\n            </Menu.Item>\r\n            <Menu.Item key=\"3\">\r\n              <Link to=\"/notes\">Notes</Link>\r\n            </Menu.Item>\r\n          </Menu>\r\n        </Header>\r\n    );\r\n};\r\n\r\nexport default AppHeader;","import React from 'react';\r\nimport '../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../index.css';\r\nimport { Layout } from 'antd';\r\n\r\nconst {Footer} = Layout;\r\n\r\nfunction AppFooter(){\r\n    return (<Footer style={{ textAlign: 'center' }}>\r\n                Mark's Blog<br></br>\r\n                Powered by React App and Ant Design Components\r\n            </Footer>);\r\n}\r\n\r\nexport default AppFooter;","const failImage = \"data:image/png;base64,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\";\r\n\r\nexport default failImage;","import React from 'react';\r\nimport '../App.css'\r\nimport 'antd/dist/antd.css';\r\nimport '../index.css';\r\nimport {Layout, Typography, Image, Space} from 'antd';\r\nimport {MailOutlined, ZhihuOutlined} from '@ant-design/icons';\r\n\r\nimport failImage from '../PublicComponent/FailImage';\r\n\r\nconst { Link, Title, Paragraph } = Typography;\r\nconst PhotoLink = process.env.PUBLIC_URL + '/Assets/';\r\n\r\nfunction AboutMe(){\r\n    return(\r\n        <Layout style={{ backgroundColor: \"white\", padding: \"0\"}}>\r\n            <Title level={2}>About Me</Title>\r\n\r\n            <Space\r\n                size=\"large\"\r\n                align=\"start\"\r\n            >\r\n\r\n            <Image\r\n                width={150}\r\n                height={150}\r\n                src={`${PhotoLink}MyPhoto.jpg`}\r\n                fallback={failImage}\r\n            />\r\n\r\n            <Paragraph>\r\n            <Title level={4}>Yutian Chen</Title>\r\n                <Paragraph copyable><MailOutlined/> markchenyutian@gmail.com</Paragraph>\r\n                ORCiD: <Link href=\"https://orcid.org/0000-0001-8008-9014\">https://orcid.org/0000-0001-8008-9014</Link>\r\n                <br></br>\r\n                <ZhihuOutlined/>  Home Page: <Link href=\"https://www.zhihu.com/people/chen-yu-tian-48-79\">https://www.zhihu.com/people/chen-yu-tian-48-79</Link>\r\n            </Paragraph>\r\n\r\n            </Space>\r\n        </Layout>\r\n    );\r\n}\r\n\r\nexport default AboutMe;","import React from 'react';\r\nimport '../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../index.css';\r\nimport {Alert, Space, Empty} from 'antd';\r\n\r\n/*\r\nThis Class `MainAlertArea` control the Alert objects on React DOM on AppMainContent.\r\nNew Alerts should be added in MainAlertArea.state.AlertList\r\n\r\nWhere there's no Alert in MainAlertArea.state.AlertList, the Empty State will be applied and rendered automatically\r\n*/\r\n\r\nclass MainAlertArea extends React.Component{\r\n    state = {\r\n        AlertList : [\r\n            <Alert\r\n            message=\"Warning\"\r\n            description=\"This Site is under Active Construction\"\r\n            type=\"warning\"\r\n            showIcon\r\n            />,\r\n\r\n            <Alert\r\n            message=\"Information\"\r\n            description=\"You can access my blog at this url: https://markchenyutian.github.io/Markchen_Blog/\"\r\n            type=\"info\"\r\n            showIcon\r\n            />\r\n        ]\r\n    };\r\n    SetEmptyArea(){\r\n        this.setState(\r\n            {\r\n                AlertList : <Empty description=\"No Info\"/>\r\n            }\r\n        );\r\n    }\r\n    render(){\r\n        if (this.state.AlertList.length === 0){\r\n            this.SetEmptyArea();\r\n        }\r\n        return (\r\n            <Space\r\n                direction=\"vertical\"\r\n                style={{width: \"100%\"}}\r\n            >\r\n                {this.state.AlertList}\r\n            </Space>\r\n        );\r\n}\r\n}\r\n\r\nexport default MainAlertArea;","import React from 'react';\r\nimport '../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../index.css';\r\nimport { Layout, Breadcrumb, Typography, Divider, Button } from 'antd';\r\nimport {ApiOutlined} from '@ant-design/icons';\r\n\r\nimport AppHeader from '../PublicComponent/Header';\r\nimport AppFooter from '../PublicComponent/Footer';\r\nimport AboutMe from './AboutMe';\r\nimport MainAlertArea from './MainAlertArea';\r\n\r\nconst {Content} = Layout;\r\nconst {Title, Paragraph, Text} = Typography;\r\n\r\n\r\nconst MainContent = () => {\r\n    window.scrollTo(0,0);\r\n    return (\r\n        <Layout>\r\n        <AppHeader select=\"1\"/>\r\n        <Content className=\"site-layout\" style={{ padding: '0 24px', marginTop: 64 }}>\r\n\r\n          <Breadcrumb style={{ margin: '16px 0' }}>\r\n            <Breadcrumb.Item>Home</Breadcrumb.Item>\r\n            <Breadcrumb.Item>Main Page</Breadcrumb.Item>\r\n          </Breadcrumb>\r\n\r\n          <div className=\"site-layout-background\" style={{ padding: 16 }}>\r\n            <AboutMe/>\r\n\r\n            <Divider></Divider>\r\n\r\n            <Title level={2}>About This App</Title>\r\n\r\n            This is the React App of Mark. Currently, the App is under construction, you can access Mark's Blog instead.\r\n            <br></br>\r\n\r\n            <Text strong>Copy the URL to access My Blog:</Text>\r\n            <Paragraph copyable>https://markchenyutian.github.io/Markchen_Blog/</Paragraph>\r\n\r\n            <Button type=\"default\" href=\"https://markchenyutian.github.io/Markchen_Blog/\">\r\n                <ApiOutlined/> Go to Mark's Blog \r\n            </Button>\r\n\r\n            <Divider orientation='left'>\r\n                Notice & Information\r\n            </Divider>\r\n            <MainAlertArea/>\r\n\r\n          </div>\r\n        </Content>\r\n        <AppFooter/>\r\n      </Layout>\r\n    );\r\n};\r\n\r\nexport default MainContent;","import React from 'react';\r\nimport '../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../index.css';\r\nimport { Layout, Breadcrumb, Typography, Space, Alert, Tag, Card, Divider } from 'antd';\r\nimport {PlusOutlined} from '@ant-design/icons';\r\nimport { Link } from 'react-router-dom';\r\n\r\nimport AppHeader from '../PublicComponent/Header';\r\nimport AppFooter from '../PublicComponent/Footer';\r\n\r\nconst {Content} = Layout;\r\nconst {Title, Paragraph} = Typography;\r\n\r\nconst MainPost = () => {\r\n    window.scrollTo(0,0);\r\n    return (\r\n        <Layout>\r\n        <AppHeader select=\"2\"/>\r\n        <Content className=\"site-layout\" style={{ padding: '0 24px', marginTop: 64 }}>\r\n\r\n          <Breadcrumb style={{ margin: '16px 0' }}>\r\n            <Breadcrumb.Item>Home</Breadcrumb.Item>\r\n            <Breadcrumb.Item>Posts</Breadcrumb.Item>\r\n          </Breadcrumb>\r\n\r\n          <div className=\"site-layout-background\" style={{ padding: 16 }}>\r\n          <Title level={2}>Posts</Title>\r\n\r\n            <Space direction=\"vertical\" size=\"middle\" style={{ width: \"100%\" }}>\r\n\r\n            <Alert\r\n                message=\"Warning\"\r\n                description=\"This Site is under Active Construction\"\r\n                type=\"warning\"\r\n                showIcon\r\n                closable\r\n            />\r\n\r\n            <Alert\r\n                message=\"Warning\"\r\n                description=\"For Better Experience, it is recommended to use PC to access this React App since using window of small width may lead to content overflow.\"\r\n                type=\"warning\"\r\n                showIcon\r\n                closable\r\n            />\r\n\r\n            <Alert\r\n                message=\"Information\"\r\n                description=\"You can access my blog at this url: https://markchenyutian.github.io/Markchen_Blog/\"\r\n                type=\"info\"\r\n                showIcon\r\n                closable\r\n            />\r\n\r\n            <Divider orientation=\"left\"><Title level={4}>Neural Network</Title></Divider>\r\n\r\n            <Card type=\"inner\" title=\"How Do Neural Network Work\" extra={<Link to=\"/posts/HowDoNeuralNetworkWork\">More <PlusOutlined /></Link>}>\r\n                <Tag color=\"blue\">Neural Network</Tag> <Tag color=\"blue\">Artificial Intelligence</Tag><Tag color=\"blue\">Machine Learning</Tag>\r\n                <Divider></Divider>\r\n                <Paragraph\r\n                ellipsis={{\r\n                    rows: 3,\r\n                    expandable: false,\r\n                }}>神经网络作为一种新兴的计算机技术被许多人称为一种全新的“编程范式”，与往常的算法编写不同，神经网络是一种“数据驱动”的编程方法。在往常的算法编写中，人们需要手动编写算法的逻辑，而在神经网络中，人们只需要为网络提供海量数据和参考答案，网络就会自动生成算法。那么神经网络到底是怎么工作的呢？</Paragraph>\r\n            </Card>\r\n\r\n            <Card type=\"inner\" title=\"What Is LSTM\" extra={<Link to=\"/posts/WhatIsLSTM\">More <PlusOutlined /></Link>}>\r\n                <Tag color=\"blue\">Neural Network</Tag> <Tag color=\"blue\">Artificial Intelligence</Tag><Tag color=\"blue\">Machine Learning</Tag>\r\n                <Divider></Divider>\r\n                <Paragraph\r\n                ellipsis={{\r\n                    rows: 3,\r\n                    expandable: false,\r\n                }}>一般的神经网络只能处理单个信息，可是有的时候神经网络的输入是一个时间序列，在这种情况下普通的前馈神经网络就不能利用“上下文”中隐含的信息来更好的处理当前输入。为了解决这个问题，人们提出了递归神经网络(Recurrent Neural Network, RNN)。可是递归神经网络也有问题：由于同样的权重在网络中一直被累乘，在反向传播的时候极容易出现梯度消失与梯度爆炸的问题。同时，由于RNN在状态间传递的信息过少，RNN在上下文距离较远的时候会很快的遗忘前文信息。为了解决这些问题，人们提出了LSTM这个新的网络模型，它可以很好的处理以上这些问题。</Paragraph>\r\n            </Card>\r\n            <Card type=\"inner\" title=\"Residual Network (ResNet)\" extra={<Link to=\"/posts/ResidualNetwork\">More <PlusOutlined /></Link>}>\r\n                <Tag color=\"blue\">Neural Network</Tag> <Tag color=\"blue\">Artificial Intelligence</Tag><Tag color=\"blue\">Machine Learning</Tag>\r\n                <Divider></Divider>\r\n                <Paragraph\r\n                ellipsis={{\r\n                    rows: 3,\r\n                    expandable: false,\r\n                }}>在深度学习中，两个严重影响了模型效果的问题是梯度消失问题与梯度下降问题。这两个问题的出现与深度学习的根本机制 - 反向传播损失函数梯度有关。在很长一段时间里，人们认为超过100层的网络是“不可训练”的。然而残差网络 (Residual Network, ResNet) 的出现改变了这一切。通过设计“短路”机制，残差网络可以让梯度更好的在网络的层之间传播，从而使得训练500+层的超深神经网络成为了可能。相似的机制也启发了一大批拥有shortcut connection的神经网络，例如在医学图像处理领域常见的 U-net 和 Dense Net。</Paragraph>\r\n            </Card>\r\n\r\n            <Divider orientation=\"left\"><Title level={4}>Artificial Intelligence</Title></Divider>\r\n\r\n            <Card type=\"inner\" title=\"What is Bayes Network\" extra={<Link to=\"/posts/WhatIsBayesNetwork\">More <PlusOutlined /></Link>}>\r\n                <Tag color=\"orange\">CS 188</Tag> <Tag color=\"blue\">Artificial Intelligence</Tag><Tag color=\"blue\">Machine Learning</Tag>\r\n                <Divider></Divider>\r\n                <Paragraph\r\n                ellipsis={{\r\n                    rows: 3,\r\n                    expandable: false,\r\n                }}>贝叶斯网络是人们在探索机器学习时的一个重要里程碑，通过贝叶斯网络，机器学习摆脱了以往基于形式逻辑推理和庞大知识库的限制，开始了“统计学习”的新纪元。那么什么是贝叶斯网络呢？贝叶斯网络和贝叶斯统计学派又有什么关系呢?</Paragraph>\r\n            </Card>\r\n            </Space>\r\n          </div>\r\n\r\n        </Content>\r\n        <AppFooter/>\r\n      </Layout>\r\n    );\r\n};\r\n\r\nexport default MainPost;","import React from 'react';\r\nimport '../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../index.css';\r\nimport { Layout, Breadcrumb, Typography, Row, Col, Tag, Card, Space } from 'antd';\r\nimport {FilePdfOutlined, BookOutlined} from '@ant-design/icons';\r\n\r\nimport AppHeader from '../PublicComponent/Header';\r\nimport AppFooter from '../PublicComponent/Footer';\r\n\r\nconst {Content} = Layout;\r\nconst {Title, Text} = Typography;\r\n\r\n\r\nconst MainNotes = () => {\r\n    window.scrollTo(0,0);\r\n    return (\r\n        <Layout>\r\n        <AppHeader select=\"3\"/>\r\n        <Content className=\"site-layout\" style={{ padding: '0 24px', marginTop: 64 }}>\r\n\r\n          <Breadcrumb style={{ margin: '16px 0' }}>\r\n            <Breadcrumb.Item>Home</Breadcrumb.Item>\r\n            <Breadcrumb.Item>Notes</Breadcrumb.Item>\r\n          </Breadcrumb>\r\n\r\n          <div className=\"site-layout-background\" style={{ padding: 16 }}>\r\n\r\n          <Title level={3}>Advanced Placement</Title>\r\n\r\n            <div className=\"site-card-wrapper\">\r\n            <Space\r\n                direction=\"vertical\"\r\n                style={{width: \"100%\"}}\r\n            >\r\n            <Row gutter={16}>\r\n            <Col span={8}>\r\n            <a href=\"https://1drv.ms/b/s!AtCdnSj9ls2qhN13evPwPUMWqidPDQ?e=lnHw6E\">\r\n                <Card title=\"Calculus\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <FilePdfOutlined style={{ fontSize: '30px'}}/>\r\n                <Text type='secondary'> Size: 50M</Text>\r\n                <Tag color=\"green\">Online</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            <Col span={8}>\r\n            <a href=\"https://1drv.ms/b/s!AtCdnSj9ls2qhN4EldJWgEmT3GQ84Q?e=A1BUpZ\">\r\n                <Card title=\"Statistics\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <FilePdfOutlined style={{ fontSize: '30px'}}/>\r\n                <Text type='secondary'> Size: 50M</Text>\r\n                <Tag color=\"green\">Online</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            <Col span={8}>\r\n            <a>\r\n                <Card title=\"CS A\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <FilePdfOutlined style={{ fontSize: '30px'}}/>\r\n                <Tag color=\"red\">N/A</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            </Row>\r\n            <Row gutter={16}>\r\n            <Col span={8}>\r\n            <a href=\"https://1drv.ms/b/s!AtCdnSj9ls2qhN12v8FiI_hO4_I3Jg?e=oFDoCn\">\r\n                <Card title=\"Biology\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <FilePdfOutlined style={{ fontSize: '30px'}}/>\r\n                <Text type='secondary'> Size: 70M</Text>\r\n                <Tag color=\"green\">Online</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            <Col span={8}>\r\n            <a href=\"https://1drv.ms/b/s!AtCdnSj9ls2qhN4FQfxa8Im_2lGjqg?e=gkU8zY\">\r\n                <Card title=\"Physics 2\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <FilePdfOutlined style={{ fontSize: '30px'}}/>\r\n                <Text type='secondary'> Size: 35M</Text>\r\n                <Tag color=\"green\">Online</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            <Col span={8}>\r\n            <a href=\"https://1drv.ms/b/s!AtCdnSj9ls2qhN4GxGkBtGwaJulLAA?e=MbHCqe\">\r\n                <Card title=\"Physics CEM\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <FilePdfOutlined style={{ fontSize: '30px'}}/>\r\n                <Text type='secondary'> Size: 5M</Text>\r\n                <Tag color=\"green\">Online</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            </Row>\r\n            <Row gutter={16}>\r\n            <Col span={8}>\r\n            <a>\r\n                <Card title=\"Chemistry\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <FilePdfOutlined style={{ fontSize: '30px'}}/>\r\n                <Tag color=\"red\">N/A</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            <Col span={8}>\r\n            <a>\r\n                <Card title=\"Microeconomics\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <FilePdfOutlined style={{ fontSize: '30px'}}/>\r\n                <Tag color=\"red\">N/A</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            <Col span={8}>\r\n            <a>\r\n                <Card title=\"Physics 1 & CM\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <FilePdfOutlined style={{ fontSize: '30px'}}/>\r\n                <Tag color=\"red\">N/A</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            </Row>\r\n            <Row gutter={16}>\r\n            <Col span={8}>\r\n            <a href=\"https://onedrive.live.com/redir.aspx?cid=aacd96fd289d9dd0&resid=AACD96FD289D9DD0!77461&parId=AACD96FD289D9DD0!104&authkey=!AL8wrY9_pFimlc\">\r\n                <Card title=\"Grade 10\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <BookOutlined style={{ fontSize: '30px'}}/>\r\n                <Text type='secondary'>OneNote Online</Text>\r\n                <Tag color=\"green\">Online</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            <Col span={8}>\r\n            <a href=\"https://onedrive.live.com/redir.aspx?cid=aacd96fd289d9dd0&resid=AACD96FD289D9DD0!77399&parId=AACD96FD289D9DD0!104&authkey=!AEkcRuZGkkj5Pe0\">\r\n                <Card title=\"Grade 11\" bordered={false} hoverable={true}>\r\n                <Space direction=\"vertical\">\r\n                <BookOutlined style={{ fontSize: '30px'}}/>\r\n                <Text type='secondary'>OneNote Online</Text>\r\n                <Tag color=\"green\">Online</Tag>\r\n                </Space>\r\n                </Card>\r\n            </a>\r\n            </Col>\r\n            </Row>\r\n            </Space>\r\n            </div>\r\n\r\n          </div>\r\n        </Content>\r\n        <AppFooter/>\r\n      </Layout>\r\n    );\r\n};\r\n\r\nexport default MainNotes;","import React from 'react';\r\nimport '../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../index.css';\r\nimport {PageHeader} from 'antd'\r\n\r\nfunction AppPageHeader(props){\r\n    return (\r\n        <PageHeader\r\n            className=\"site-page-header\"\r\n            onBack={() => window.history.back()}\r\n            title={props.title}\r\n            subTitle={props.subTitle}\r\n        />\r\n    );\r\n}\r\n\r\nexport default AppPageHeader;","import React from 'react';\r\nimport '../../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../../index.css';\r\nimport { Image, Layout, Typography, Space, Tag, Divider } from 'antd';\r\n\r\n\r\nimport { InlineMath, BlockMath } from 'react-katex'\r\nimport 'katex/dist/katex.min.css';\r\nimport SyntaxHighlighter from 'react-syntax-highlighter';\r\nimport { lightfair } from 'react-syntax-highlighter/dist/esm/styles/hljs';\r\n\r\nimport AppHeader from '../../PublicComponent/Header';\r\nimport AppFooter from '../../PublicComponent/Footer';\r\nimport FailImage from '../../PublicComponent/FailImage';\r\nimport AppPageHeader from '../../PublicComponent/PageHeader';\r\n\r\nconst { Title, Text, Paragraph } = Typography;\r\nconst { Content } = Layout\r\nconst PhotoLink = process.env.PUBLIC_URL + '/Assets/'\r\n\r\nfunction HowDoNeuralNetworkWork(props){\r\n    window.scrollTo(0,0);\r\n    return(\r\n<Layout>\r\n        <AppHeader select=\"2\"/>\r\n        <Content className=\"site-layout\" style={{ padding: '0 24px', marginTop: 64 }}>\r\n        <AppPageHeader title=\"How Do Neural Network Work\"/>\r\n          <div className=\"site-layout-background\" style={{ padding: 16 }}>\r\n              <PostContent/>\r\n          </div>\r\n        </Content>\r\n        <AppFooter/>\r\n      </Layout>\r\n    );\r\n}\r\n\r\nexport default HowDoNeuralNetworkWork;\r\n\r\nfunction PostContent(){\r\n    return (\r\n        <Layout style={{ backgroundColor: \"white\", padding: \"0\"}}>\r\n        <div>\r\n        <Tag color=\"blue\">Neural Network</Tag> <Tag color=\"blue\">Artificial Intelligence</Tag><Tag color=\"blue\">Machine Learning</Tag>\r\n        </div>\r\n        <Divider></Divider>\r\n        <Paragraph>神经网络作为一种新兴的计算机技术被许多人称为一种全新的“编程范式”，与往常的算法编写不同，神经网络是一种“数据驱动”的编程方法。在往常的算法编写中，人们需要手动编写算法的逻辑，而在神经网络中，人们只需要为网络提供海量数据和参考答案，网络就会自动生成算法。那么神经网络到底是怎么工作的呢？</Paragraph>\r\n\r\n        <Paragraph>这篇文章会对机器学习中的神经网络为什么可以被训练&输出正确预测做出<Text strong>不严谨但直观</Text>的解释。</Paragraph>\r\n\r\n        <Title level={3}>0. 模型是一个函数</Title>\r\n        <Paragraph>我们可以将一个深度学习中的模型看做一个映射关系：</Paragraph>\r\n        <BlockMath math=\"\\text{Perception} \\rightarrow \\text{Output}\"/>\r\n        <Paragraph>对于一个深度学习模型是“感知”（模型可以获得的所有信息的总和）与一个“数字”或者 “决策\"之间的映射关系。所以我们可以将模型看作一个函数<InlineMath math=\"F(x)\"/>.</Paragraph>\r\n        <Paragraph>那么模型就可以被表示为：<InlineMath math=\"F(\\text{Perception}) =\\text{Output}\"/></Paragraph>\r\n        <Paragraph type=\"secondary\">Example: Alpha Go 可以被表示为 <InlineMath math=\"F(\\text{Chess State}) = \\text{Best Position for Next Chess}\"/> 这样一个函数</Paragraph>\r\n        <Paragraph>现在我们假设有这样的一个函数：对于<Text strong>任何定义域内的输入都一定会给出此时的最优输出</Text>。这样的一个理想函数我们记作<InlineMath math=\"G(x)\"/>(Ground Truth)。 当我们“训练”模型<InlineMath math=\"F(x)\"/>的时候，我们的目标就是让模型尽可能拟合<InlineMath math=\"G(x)\"/>。也就是说，我们想要通过训练使得我们的模型<InlineMath math=\"F(x)\"/> 的输出与事实（最优函数）<InlineMath math=\"G(x)\"/>的差距最小化。</Paragraph>\r\n\r\n        <Title level={3}>1. 什么是神经网络</Title>\r\n        <Paragraph>要知道为什么”神经网络“可以被用来拟合函数呢？首先我们先了解一下什么是“神经网络”。</Paragraph>\r\n        <Paragraph>神经网络由许多神经元相互连接而组成，每个神经元都有自己的参数<InlineMath math=\"\\theta\"/> 。我们可以将神经元描绘为一个函数 <InlineMath math=\" f(\\theta_i, x) = y\"/>。那么对于下面一个模型（<InlineMath math=\"F(\\Theta, x), \\quad \\Theta=\\lbrace \\theta_1, \\theta_2, \\dots, \\theta_n\\rbrace\"/>），我们可以写出它的数学表达式：</Paragraph>\r\n        <center><Image\r\n            width=\"350px\"\r\n            src={`${PhotoLink}HowDoNeuralNetworkWork4.png`}\r\n            fallback={FailImage}\r\n        /></center>\r\n        <BlockMath math=\"F(\\Theta, x) = f(\\theta_5, (f(\\theta_3, f(\\theta_2, x_2) + f(\\theta_1, x_1)), f(\\theta_4, f(\\theta_2, x_2))))\"/>\r\n\r\n        <Title level={3}>2. 神经网络可以拟合函数</Title>\r\n        <Paragraph>神经网络的本质建立在这样一个事实上：简单非线性函数的重复的迭代与叠加可以在拥有适当参数的情况下<Text strong>有限精度的拟合任何连续函数</Text>。下面的例子会给出一个<Text type=\"warning\">直观但不严谨的</Text>，对神经网络拟合二元函数的证明：</Paragraph>\r\n        <Paragraph>首先，我们可以用5个使用sigmoid函数的神经元来构建一个“高台”函数。(代码是具体的实现)</Paragraph>\r\n        <center><Space>\r\n        <Image\r\n            width=\"200px\"\r\n            src={`${PhotoLink}HowDoNeuralNetworkWork1.png`}\r\n            fallback={FailImage}\r\n        />\r\n        <Image \r\n            width=\"200px\"\r\n            src={`${PhotoLink}HowDoNeuralNetworkWork3.png`}\r\n            fallback={FailImage}\r\n        />\r\n        </Space></center>\r\n        <SyntaxHighlighter language=\"python\" style={lightfair}\r\n        children={\r\n            `\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport pylab\r\nfrom matplotlib import cm\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\n\r\ndef sigmoid(x):\r\n    s = 1 / (1 + np.exp(-x))\r\n    return s\r\n\r\ndef tower(x, y, x_min, x_max, y_min, y_max):\r\n    x1 = sigmoid(1000 * (x - x_min))\r\n    x2 = sigmoid(1000 * (x - x_max))\r\n\r\n    y1 = sigmoid(1000 * (y - y_min))\r\n    y2 = sigmoid(1000 * (y - y_max))\r\n\r\n    z = x1-x2+y1-y2\r\n    z = sigmoid(30*(z-1.1))\r\n    return z\r\n\r\n\r\nX = np.arange(-5, 5, 0.1)\r\nY = np.arange(-5, 5, 0.1)\r\nX, Y = np.meshgrid(X, Y)\r\nZ = tower(X, Y, -0.3, 0.7, -0.2, 0.8)\r\nfig = plt.figure()\r\nax = Axes3D(fig)\r\nax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.viridis)\r\nplt.show()\r\n            `\r\n        }\r\n        />\r\n        <Paragraph>如果我们把这样的一个高台记作<InlineMath math=\"Tower(x_1, x_2,\\Theta)\"/>，那么通过组合足够多这些高台，我们可以得到任何一个连续二元函数的任意小精度拟合（缩小每个高台的面积），例如下图（左：原函数，右：四个<InlineMath math=\"Tower(x_1, x_2,\\Theta)\"/>的组合</Paragraph>\r\n        <center><Image\r\n            width=\"350px\"\r\n            src={`${PhotoLink}HowDoNeuralNetworkWork2.png`}\r\n            fallback={FailImage}\r\n        /></center>\r\n        <SyntaxHighlighter\r\n        style={lightfair}\r\n        language=\"python\"\r\n        children={\r\n        `\r\ndef tower(x, y, x_min, x_max, y_min, y_max):\r\n    x1 = sigmoid(1000 * (x - x_min))\r\n    x2 = sigmoid(1000 * (x - x_max))\r\n\r\n    y1 = sigmoid(1000 * (y - y_min))\r\n    y2 = sigmoid(1000 * (y - y_max))\r\n\r\n    z = x1-x2+y1-y2\r\n    z = sigmoid(4*(z-1.1))\r\n    return z\r\n\r\nZ = tower(X, Y, -0.5, 0.5, -0.5, 0.5) + tower(X, Y, -1, 1, -1, 1) + tower(X, Y, -2, 2, -2, 2) + tower(X, Y, -4, 4, -4, 4)\r\nfig = plt.figure()\r\nax = Axes3D(fig)\r\nax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.viridis)\r\nplt.show()\r\n        `\r\n        }\r\n        />\r\n\r\n        <Title level={3}>3. 如何让电脑自动调参？</Title>\r\n        <Paragraph>在上面的例子中，所有的参数都是人工设定的，因为只有20个不到的参数，人工设定是一种可行的做。可是目前绝大多数的模型都有超过一万个参数，参数最多的自然语言模型GPT-3甚至有1730亿个参数（存储整个模型需要800T空间）！在这么多参数的情况下，人工调节每一个参数变成了一项不可能的任务，所以我们需要让电脑来自动调整参数来让模型<InlineMath math=\"F(x)\"/>拟合到目标<InlineMath math=\"G(x)\"/>上。</Paragraph>\r\n        <Paragraph>要让电脑自动完成这项工作，我们需要先回想一下当我们调整参数时我们所作的工作：1. 评估现在的模型<InlineMath math=\"F(x)\"/>与<InlineMath math=\"G(x)\"/>相差大不大（现在的模型是不是一个好模型）2. 预测调节参数<InlineMath math=\"\\theta\"/>（调大/调小）以后模型会变好还是变坏 3. 如果参数<InlineMath math=\"\\theta\"/>调小可以让模型<InlineMath math=\"F(x)\"/>更加接近<InlineMath math=\"G(x)\"/>，那么就调小<InlineMath math=\"\\theta\"/>， 反之亦然</Paragraph>\r\n        <Title level={4}>损失函数</Title>\r\n        <Paragraph>为了让机器拥有完成任务1的能力，人们设计出了“损失函数”用来量化表示模型<InlineMath math=\"F(x)\"/>与事实<InlineMath math=\"G(x)\"/>之间的差距，用<InlineMath math=\"L(\\hat{y}, y)\"/>表示，<InlineMath math=\"\\hat{y}\"/>表示模型的输出（对Ground Truth <InlineMath math=\"y\"/>的预测值），一般来说，一个良好的损失函数应该有这些性质：</Paragraph>\r\n\r\n        <Paragraph>1. 损失函数大小与模型质量单调递增 - 模型越差，损失函数越大\r\n        2. 损失函数应该是一个连续，尽量平滑的函数</Paragraph>\r\n\r\n        <Paragraph>一种常见的损失函数是<InlineMath math=\"L(\\hat{y}, y) = (y - \\hat{y})^2\"/></Paragraph>\r\n\r\n        <Title level={4}>参数调节方向的计算</Title>\r\n        <Paragraph>为了让机器完成任务2 和 3，我们需要将”预测调节参数<InlineMath math=\"\\theta\" />（调大/调小）以后模型会变好还是变坏“这样一个主观的过程用数学方法表达出来。因为我们已经引入了损失函数，所以实际上这个过程可以被表述为“预测如何调节参数<InlineMath math=\"\\theta\"/>（调大/调小）可以减小损失函数的值”</Paragraph>\r\n\r\n        <Paragraph>在此之前，我们先看一看我们如何最小化一个一元函数<InlineMath math=\"h(x)\"/>. 对于一个一元函数，我们可以计算出当前位置的一阶导数<InlineMath math=\"dh/dx\"/>。如果一阶导数是正数，说明增大<InlineMath math=\"x\"/>可以增大<InlineMath math=\"h(x)\"/>，反之亦然。所以要最小化<InlineMath math=\"h(x)\"/>，我们只需要不停的执行下面这一个操作：</Paragraph>\r\n        <BlockMath math=\"x\\stackrel{\\text{update}}{\\longrightarrow}x - \\eta \\cdot \\frac{dh(x)}{dx},\\quad\\quad \\text{where $\\eta$ is a positive number}\"/>\r\n        <Paragraph>这里的<InlineMath math=\"\\eta\"/>是一个参数“学习速率”，学习速率越高，每次更新<InlineMath math=\"x\"/>的时候<InlineMath math=\"x\"/>的值就会改变越多 。</Paragraph>\r\n        <Paragraph>有了上面的铺垫，解决“预测如何调节参数<InlineMath math=\"\\theta\"/>（调大/调小）可以减小损失函数的值”的方法就很明显了：计算<InlineMath math=\"\\partial L(\\hat{y}, y)/\\partial \\theta\"/> 并且将<InlineMath math=\"\\theta\"/>按照一下方式更新：</Paragraph>\r\n        <BlockMath math=\"\\theta\\stackrel{\\text{update}}{\\longrightarrow}\\theta - \\eta \\cdot \\frac{\\partial L(\\hat{y}, y)}{\\partial\\theta},\\quad\\quad \\text{where $\\eta$ is a positive number}\"/>\r\n        <Paragraph type=\"secondary\"> 有些人可能会疑惑，在<InlineMath math=\"L(\\hat{y}, y)\"/>中明明都没有自变量 <InlineMath math=\"\\theta\"/> 啊，怎么计算<InlineMath math=\"\\frac{\\partial L(\\hat{y}, y)}{\\partial \\theta}\"/> 呢？</Paragraph>\r\n\r\n        <Paragraph type=\"secondary\">实际上注意到损失函数的第一个输入时<InlineMath math=\"\\hat{y}\"/>，也就是模型的输出，而模型可以表示为<InlineMath math=\"F(\\theta, x)\"/>，所以我们可以通过<Text strong>链式法则</Text>计算<InlineMath math=\"\\frac{\\partial L(\\hat{y}, y)}{\\partial\\theta}\"/>\r\n        <BlockMath math=\"\\frac{\\partial L(\\hat{y}, y)}{\\partial \\theta} = \\frac{\\partial L(\\hat{y}, y)}{\\partial \\hat{y}}\\cdot \\frac{\\partial \\hat{y}}{\\partial \\theta}\"/>\r\n        这也是神经网络的基石 - 反向传播算法 (Back Propagation) 的数学原理\r\n        </Paragraph>\r\n        <Paragraph>当机器拥有了自动更新权重的能力的时候，我们就可以开始对神经网络进行训练了！训练的过程其实就是将样本从训练数据集中输入到模型中，再通过算法自动调节模型函数来最小化损失函数。</Paragraph>\r\n        </Layout>\r\n    );\r\n}\r\n","import React from 'react';\r\nimport '../../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../../index.css';\r\nimport { Image, Layout, Typography, Tag, Divider } from 'antd';\r\n\r\nimport { InlineMath, BlockMath } from 'react-katex'\r\nimport 'katex/dist/katex.min.css';\r\n\r\nimport AppHeader from '../../PublicComponent/Header';\r\nimport AppFooter from '../../PublicComponent/Footer';\r\nimport FailImage from '../../PublicComponent/FailImage';\r\nimport AppPageHeader from '../../PublicComponent/PageHeader';\r\n\r\nconst { Title, Text, Paragraph, Link } = Typography;\r\nconst { Content } = Layout\r\nconst PhotoLink = process.env.PUBLIC_URL + '/Assets/'\r\n\r\nfunction WhatIsLSTM(){\r\n    window.scrollTo(0,0);\r\n    return(\r\n        <Layout>\r\n        <AppHeader select=\"2\"/>\r\n        <Content className=\"site-layout\" style={{ padding: '0 24px', marginTop: 64 }}>\r\n        <AppPageHeader title=\"What is LSTM\"/>\r\n          <div className=\"site-layout-background\" style={{ padding: 16 }}>\r\n              <PostContent/>\r\n          </div>\r\n        </Content>\r\n        <AppFooter/>\r\n      </Layout>\r\n    );\r\n}\r\n\r\nexport default WhatIsLSTM;\r\n\r\nfunction PostContent(){\r\n    return(\r\n    <Layout style={{ backgroundColor: \"white\", padding: \"0\"}}>\r\n        <div>\r\n        <Tag color=\"blue\">Neural Network</Tag> <Tag color=\"blue\">Artificial Intelligence</Tag> <Tag color=\"blue\">Marhine Learning</Tag>\r\n        <Divider></Divider>\r\n        </div>\r\n        <Paragraph>一般的神经网络只能处理单个信息，可是有的时候神经网络的输入是一个时间序列，在这种情况下普通的前馈神经网络就不能利用“上下文”中隐含的信息来更好的处理当前输入。为了解决这个问题，人们提出了递归神经网络(Recurrent Neural Network, RNN)。可是递归神经网络也有问题：由于同样的权重在网络中一直被累乘，在反向传播的时候极容易出现梯度消失与梯度爆炸的问题。同时，由于RNN在状态间传递的信息过少，RNN在上下文距离较远的时候会很快的遗忘前文信息。为了解决这些问题，人们提出了LSTM这个新的网络模型，它可以很好的处理以上这些问题。</Paragraph>\r\n        <Title level={2}>LSTM - 过去，现在，和未来</Title>\r\n        <Paragraph><Text strong>0. 什么是LSTM</Text></Paragraph>\r\n        <Paragraph><Text strong>1. 为什么需要LSTM</Text></Paragraph>\r\n        <Paragraph><Text strong>2. LSTM的直觉解释</Text></Paragraph>\r\n        <Paragraph><Text strong>3. LSTM的具体解释</Text></Paragraph>\r\n        <Paragraph><Text strong>4. LSTM的变体</Text></Paragraph>\r\n        <Paragraph><Text strong>5. 参考资料</Text></Paragraph>\r\n        <hr />\r\n        <Title level={3}>0. 什么是LSTM</Title>\r\n        <Paragraph>LSTM，全称 Long Short Term Memory (长短期记忆) 是一种特殊的<Text strong>递归神经网络</Text> 。这种网络 \r\n        与一般的前馈神经网络不同，LSTM可以利用时间序列对输入进行分析；简而言之，当使用前馈神经网络时，神经网络会认为我们<InlineMath math=\"t\"/>时刻输入的内容与<InlineMath math=\"t + 1\"/>时刻输入的内容<Text strong>完全无关</Text>，对于许多\r\n        情况，例如图片分类识别，这是毫无问题的，可是对于一些情景，例如<Text strong>自然语言处理</Text> (NLP, Natural Language Processing) 或者我们需要分析类似于<Text strong>连拍照片</Text>这样的数据时，合理运用 <InlineMath math=\"t\"/> 或\r\n        之前的输入来处理 <InlineMath math=\"t+n\"/> 时刻显然可以更加合理的运用输入的信息。为了运用到时间维度上信息，人们设计了<Text strong>递归神经网络</Text> (RNN, Recurssion Neural Network)，一个简单的递归神经网络可以用这种方式表示</Paragraph>\r\n        <center><Image alt=\"image-20200402223614052\" src={`${PhotoLink}LSTM1.png`} width=\"200px\" fallback={FailImage}/></center>\r\n        <Paragraph>在图中，<InlineMath math=\"x_t\"/>是在<InlineMath math=\"t\"/>时刻的输入信息，<InlineMath math=\"h_t\"/>是在<InlineMath math=\"t\"/>时刻的输入信息，我们可以看到神经元<InlineMath math=\"A\"/>会递归的调用自身并且将<InlineMath math=\"t -1\"/>时刻的信息传递给<InlineMath math=\"t\"/>时刻。</Paragraph>\r\n        <Paragraph>递归神经网络在许多情况下运行良好，特别是在对<Text strong>短时间序列</Text>数据的分析时十分方便。但是， \r\n        注意到前面着重强调了“短”，这是为什么呢？</Paragraph>\r\n        <Paragraph>上图所示的简单递归神经网络存在一个“硬伤“，<Text strong>长期依赖问题</Text>：递归神经网络只能处理我们需 \r\n        要较接近的上下文的情况：</Paragraph>\r\n        <Paragraph type=\"secondary\">\r\n        <Paragraph>Example 1. 想象现在设计了一个基于简单RNN的句子自动补全器，当我输入\"Sea is ...\" 的时候会自动补全为\"Sea is <Text strong>blue</Text>\"。</Paragraph>\r\n        </Paragraph>\r\n        <Paragraph>在这种情况下，我们需要的上下文极短，而RNN可以很好的收集到 <InlineMath math=\"t = 0\"/>时的信息\"Sea\"并且补\r\n        上\"blue\"</Paragraph>\r\n        <Paragraph type=\"secondary\">\r\n        <Paragraph>Example 2. 现在，假设我们用刚刚的RNN试图补全一篇文章\"我一直呆在中国，……，我会说一口流利的 (?)\"。</Paragraph>\r\n        </Paragraph>\r\n        <Paragraph>在这里，为了补全最后的空缺，需要的信息在非常远的上文（e.g. 200+字前）提到的”中国“。在实验中简单的理想状\r\n        态下，经过精心调节的RNN超参数可以良好的将这些信息向后传递。可是在现实的情况中，基本没有RNN可以做到这一点。一些学者\r\n        后来研究发现RNN的长期依赖问题是这种网络结构本身的问题。</Paragraph>\r\n        <Paragraph>不但如此，这种简单的RNN还很容易受到两种在神经网络中臭名昭著的影响<Text strong>梯度消失问题</Text>（神经\r\n        网络的权重/偏置梯度极小，导致神经网络参数调整速率急剧下降）和<Text strong>梯度爆炸问题</Text>（神经网络的权重/偏置\r\n        极大，导致神经网络参数调整幅度过大，矫枉过正）。相信大家都看过一个著名的鸡汤，<InlineMath math=\"(0.99)^{365}\"/>和<InlineMath math=\"(1.01)^{365}\"/>的对比。实际上，这个鸡汤非常好的描述了梯度问题的本质：对于<Text strong>任意信息递 \r\n        归使用足够多次同样的计算</Text>，都会导致极大或极小的结果，也就是说…</Paragraph>\r\n        <Paragraph>根据微分链式法则，在RNN中，神经元的权重的梯度可以被表示为一系列函数的微分的连乘。因为神经元的参数（权重\r\n        与偏置）都是基于学习速率（一般为常数）和参数梯度相反数（使得神经网络输出最快逼近目标输出）得到的，一个过大或过小的\r\n        梯度会导致我们要么需要极长的训练时间（本来从-2.24 调节到 -1.99 只用500个样本，由于梯度过小，每次只调小0.0001，最后\r\n        用了几千个样本），要么会导致参数调节过度（例如本来应该从-10.02调节到-9.97，由于梯度过大，直接调成了+20.3）</Paragraph>\r\n        <Title level={3}>1. 为什么需要LSTM</Title>\r\n        <Paragraph>LSTM从被设计之初就被明确的用于解决一般递归神经网络中普遍存在的<Text strong>长期依赖问题</Text>，使用LSTM可以有效的传递和表达长时间序列中的信息并且不会导致长时间前的有用信息被忽略（遗忘）。与此同时，LSTM还可以解决RNN中\r\n        的梯度消失/爆炸问题</Paragraph>\r\n        <Title level={3}>2. LSTM 的直觉解释</Title>\r\n        <Paragraph>LSTM的设计或多或少的借鉴了人类对于自然语言处理的直觉性经验。要想了解LSTM的工作机制，可以先阅读一下一个 \r\n        （虚构的）淘宝评论：</Paragraph>\r\n        <Paragraph type=\"secondary\">\r\n        <Paragraph>“这个笔记本非常棒，纸很厚，料很足，用笔写起来手感非常舒服，而且没有一股刺鼻的油墨味；更加好的是这个笔记\r\n        本不但便宜还做工优良，我上次在别家买的笔记本裁纸都裁不好，还会割伤手……”</Paragraph>\r\n        </Paragraph>\r\n        <Paragraph>如果让你看完这段话以后马上转述，相信大多数人都会提取出来这段话中几个重要的关键词“纸好”，“没味道”，“做工\r\n        好”，然后再重新组织成句子进行转述。这说明了以下两点：</Paragraph>\r\n        <ol>\r\n        <li>在一个时间序列中，不是所有信息都是同等有效的，大多数情况存在“关键词”或者“关键帧”</li>\r\n        <li>我们会在从头到尾阅读的时候“自动”概括已阅部分的内容并且用之前的内容帮助理解后文</li>\r\n        </ol>\r\n        <Paragraph>基于以上这两点，LSTM的设计者提出了“长短期记忆”的概念——只有一部分的信息需要长期的记忆，而有的信息可以不 \r\n        记下来</Paragraph>\r\n        <Title level={3}>3. LSTM的具体解释</Title>\r\n        <Paragraph>一个普通的，使用tanh函数的RNN可以这么表示：</Paragraph>\r\n        <center><Image alt=\"image-20200402233238756\" src={`${PhotoLink}LSTM2.png`} width=\"50%\" fallback={FailImage}/></center>\r\n        <Paragraph>在这里，我们可以看到A在<InlineMath math=\"t-1\"/>时刻的输出值<InlineMath math=\"h_t\"/>被复制到了<InlineMath math=\"t\"/>时刻，与<InlineMath math=\"t\"/>时刻的输入<InlineMath math=\"x_t\"/>整合后经过一个带权重和偏置的tanh函数后\r\n        形成输出，并继续将数据复制到<InlineMath math=\"t+1\"/>时刻……</Paragraph>\r\n        <Paragraph>与上图朴素的RNN相比，单个LSTM单元拥有更加复杂的内部结构和输入输出：</Paragraph>\r\n        <center><Image alt=\"image-20200402233826864\" src={`${PhotoLink}LSTM3.png`} width=\"50%\" fallback={FailImage}/></center>\r\n        <Paragraph>在上图中，每一个红色圆形代表对向量做出的操作（pointwise operation， 对位操作），而黄色的矩形代表一个神 \r\n        经网络层，上面的字符代表神经网络所使用的激活函数</Paragraph>\r\n        <Paragraph type=\"secondary\">\r\n        <Paragraph>point-wise operation 点对点操作</Paragraph>\r\n        <Paragraph>​ 如果我要对向量&lt;1, 2, 3&gt; 和 &lt;1, 3, 5&gt;进行逐分量的想成操作，会获得结果 &lt;1, 6, 15&gt;</Paragraph>\r\n        <Paragraph>layer 函数层</Paragraph>\r\n        <Paragraph>​ 一个函数层拥有两个属性：权重向量(Weight) 和 偏置向量(bias)，对于输入向量<InlineMath math=\"A\"/>的每一 \r\n        个分量 <InlineMath math=\"i\"/> ， 函数  层会对其进行以下操作(假设激活函数为<InlineMath math=\"F(x)\"/>)：\r\n        <BlockMath math=\"\r\n        Output_i = F(W_i \\cdot A_i + b_i)\r\n        \"/>\r\n        ​ 常见的激活函数（也就是套在最外面的<InlineMath math=\"F(x)\"/>）有ReLU(线性修正单元)，sigmoid（写作<InlineMath math=\"\\sigma\"/>），和 <InlineMath math=\"    anh\"/></Paragraph>\r\n        </Paragraph>\r\n        <Title level={4}>LSTM的关键：单元状态</Title>\r\n        <Paragraph>LSTM能够从RNN中脱颖而出的关键就在于上图中从单元中贯穿而过的线 ——神经元的隐藏态，我们可以将神经元的隐藏 \r\n        态简单的理解成递归神经网络对于输入数据的“记忆”，用<InlineMath math=\"C_t\"/>表示神经元在<InlineMath math=\"t\"/>时刻过\r\n        后的“记忆”，这个向量涵盖了在<InlineMath math=\"t+1\"/>时刻前神经网络对于所有输入信息的“概括总结”</Paragraph>        \r\n        <center><Image alt=\"image-20200402235227710\" src={`${PhotoLink}LSTM4.png`} width=\"50%\" fallback={FailImage}/></center>\r\n        <Paragraph>接下来我们会看一下LSTM四个函数层分别在做些什么</Paragraph>\r\n        <Title level={4}>LSTM_1 遗忘门</Title>\r\n        <center><Image alt=\"image-20200403003547037\" src={`${PhotoLink}LSTM5.png`} width=\"50%\" fallback={FailImage}/></center>\r\n        <Paragraph>对于上一时刻LSTM中的单元状态来说，一些“信息”可能会随着时间的流逝而“过时”。为了不让过多记忆影响神经网络 \r\n        对现在输入的处理，我们应该选择性遗忘一些在之前单元状态中的分量——这个工作就交给了“遗忘门”</Paragraph>\r\n        <Paragraph>每一次输入一个新的输入，LSTM会先根据新的输入和上一时刻的输出决定遗忘掉之前的哪些记忆——输入和上一步的输 \r\n        出会整合为一个单独的向量，然后通过sigmoid神经层，最后点对点的乘在单元状态上。因为sigmoid 函数会将任意输入压缩到<InlineMath math=\"(0, 1)\"/>的区间上，我们可以非常直觉的得出这个门的工作原理 —— 如果整合后的向量某个分量在通过sigmoid \r\n        层后变为0，那么显然单元状态对应的分量也会变成0，换句话说，“遗忘”了这个分量上的信息；如果某个分量通过sigmoid层后为1，单元状态会“保持完整记忆”。不同的sigmoid输出会带来不同信息的记忆与遗忘。通过这种方式，LSTM可以<Text strong>长期记\r\n        忆重要信息</Text>，并且记忆可以随着时间进行动态调整</Paragraph>\r\n        <Paragraph>下面的公式可以用来描述遗忘门的计算，其中<InlineMath math=\"f_t\"/>就是sigmoid神经层的输出向量：\r\n        <BlockMath math=\"\r\n        f_t = \\sigma(W_f\\cdot [h_{t-1}, x_t] + b_f)\r\n        \"/></Paragraph>\r\n        <Title level={4}>LSTM_2 &amp; 3 记忆门</Title>\r\n        <Paragraph>记忆门是用来控制是否将在<InlineMath math=\"t\"/>时刻（现在）的数据并入单元状态中的控制单位。首先，用tanh \r\n        函数层将现在的向量中的有效信息提取出来，然后使用（图上tanh函数层左侧）的sigmoid函数来控制这些记忆要放“多少”进入单 \r\n        元状态。这两者结合起来就可以做到：</Paragraph>\r\n        <center><Image alt=\"image-20200403001917424\" src={`${PhotoLink}LSTM6.png`} width=\"50%\" fallback={FailImage}/></center>\r\n        <ol>\r\n        <li>从当前输入中提取有效信息</li>\r\n        <li>对提取的有效信息做出筛选，为每个分量做出评级(0 ~ 1)，评级越高的最后会有越多的记忆进入单元状态</li>\r\n        </ol>\r\n        <Paragraph>下面的公式可以分别表示这两个步骤在LSTM中的计算：</Paragraph>\r\n        <ol>\r\n        <li>\r\n        <Paragraph><BlockMath math=\"\r\n        C'_t = \\tanh(W_c\\cdot [h_{t - 1},x_t] + b_c)\r\n        \"/></Paragraph>\r\n        </li>\r\n        <li>\r\n        <Paragraph><BlockMath math=\"\r\n        i_t = \\sigma(W_i\\cdot [h_{t-1}, x_t] + b_i)\r\n        \"/></Paragraph>\r\n        </li>\r\n        </ol>\r\n        <Title level={4}>LSTM_4 输出门</Title>\r\n        <Paragraph>输出门，顾名思义，就是LSTM单元用于计算当前时刻的输出值的神经层。输出层会先将当前输入值与上一时刻输出值 \r\n        整合后的向量（也就是公式中的<InlineMath math=\"[h_{t - 1},x_t]\"/>）用sigmoid函数提取其中的信息，接着，会将当前的单 \r\n        元状态通过tanh函数压缩映射到区间<InlineMath math=\"(-1, 1)\"/>中*</Paragraph>\r\n        <Paragraph type=\"secondary\">\r\n        <Paragraph><Text strong>为什么我们要在LSTM的输出门上使用tanh函数？</Text></Paragraph>\r\n        <Paragraph>以下引用自Stack Overflow上问题 What is the intuition of using tanh in LSTM 中的最佳答案：</Paragraph>  \r\n        <Link href=\"https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm\" >https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm</Link>   \r\n        <Paragraph>在LSTM的输入和输出门中使用tanh函数有以下几个原因：</Paragraph>\r\n        <ol>\r\n        <li>为了防止<Text strong>梯度消失问题</Text>，我们需要一个二次导数在大范围内不为0的函数，而tanh函数可以满足\r\n        这一点</li>\r\n        <li>为了便于凸优化，我们需要一个<Text strong>单调函数</Text></li>\r\n        <li>tanh函数一般收敛的更快</li>\r\n        <li>tanh函数的求导占用系统的资源更少</li>\r\n        </ol>\r\n        </Paragraph>\r\n        <Paragraph>将经过tanh函数处理后的单元状态与sigmoid函数处理后的，整合后的向量点对点的乘起来就可以得到LSTM在<InlineMath math=\"t\"/>时刻的输出了！</Paragraph>\r\n        <hr />\r\n        <Title level={3}>4. LSTM 的变体</Title>\r\n        <Paragraph>自从LSTM在自然语言处理等方面大获成功后，许多种LSTM的变体被提出，其中只有几种值得特别关注：</Paragraph> \r\n        <center><Image alt=\"image-20200403021009010\" src={`${PhotoLink}LSTM7.png`} width=\"50%\" fallback={FailImage}/></center>\r\n        <Paragraph>这种LSTM让各个门都可以在获得了上一时刻的单元状态的前提下进行运算。在上面的图中，单元状态被额外赋予到了 \r\n        所有三个层中（输出门除外），然而在实际的应用中，大部分研究者只会选择性的打开三个通道中的一或两个</Paragraph>      \r\n        <Paragraph>除此之外，还有很多其他LSTM变体以及<Text strong>通过其他方式构建RNN达到类似LSTM的效果的架构</Text>，然而\r\n        这些架构的效率都大同小异，所以不过多说明了</Paragraph>\r\n        <Title level={3}>5. 参考资料</Title>\r\n        <Paragraph>[1] “Understanding LSTM Networks.” <em>Understanding LSTM Networks -- Colah's Blog</em>, colah.github.io/posts/2015-08-Understanding-LSTMs/.</Paragraph>\r\n        <Paragraph>[2] “Long Short-Term Memory.” <em>Wikipedia</em>, Wikimedia Foundation, 1 Apr. 2020, en.wikipedia.org/wiki/Long_short-term_memory.</Paragraph>\r\n        <Paragraph>[3] “LSTM以及三重门，遗忘门，输入门，输出门.” <em>LSTM以及三重门，遗忘门，输入门，输出门_网络_Lison_Zhu's Blog-CSDN博客</em>, blog.csdn.net/Lison_Zhu/article/details/97236501.</Paragraph>\r\n        <Paragraph>[4] “递归神经网络问题整理.” <em>递归神经网络问题整理_网络_leo鱼的博客-CSDN博客</em>, blog.csdn.net/webzjuyujun/article/details/71124695.</Paragraph>\r\n        <Paragraph>[5] “详解机器学习中的梯度消失、爆炸原因及其解决方法.” <em>详解机器学习中的梯度消失、爆炸原因及其解决方 \r\n        法_网络_Double_V的博客-CSDN博客</em>, blog.csdn.net/qq_25737169/article/details/78847691.</Paragraph>\r\n        <Paragraph>[6] Dnkdnk. “What Is the Intuition of Using Tanh in LSTM.” <em>Stack Overflow</em>, 1 Sept. 1966, stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm.</Paragraph>\r\n        </Layout>);\r\n}","import React from 'react';\r\nimport '../../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../../index.css';\r\nimport { Image, Layout, Typography, Tag, Divider } from 'antd';\r\nimport { InlineMath } from 'react-katex';\r\nimport 'katex/dist/katex.min.css';\r\nimport AppHeader from '../../PublicComponent/Header';\r\nimport AppFooter from '../../PublicComponent/Footer';\r\nimport FailImage from '../../PublicComponent/FailImage';\r\nimport AppPageHeader from '../../PublicComponent/PageHeader';const { Title, Text, Paragraph } = Typography;\r\nconst { Content } = Layout;\r\nconst PhotoLink = process.env.PUBLIC_URL + '/Assets/';\r\nfunction ResidualNetwork(){\r\n   window.scrollTo(0,0);\r\n   return(\r\n       <Layout>\r\n           <AppHeader select='2'/>\r\n           <Content className=\"site-layout\" style={{ padding: '0 24px', marginTop: 64 }}>\r\n           <AppPageHeader title='Residual Network'/>\r\n           <div className='site-layout-background' style={{ padding: 16 }}>\r\n              <PostContent/>\r\n           </div>\r\n           </Content>\r\n           <AppFooter/>\r\n        </Layout>\r\n   );\r\n}\r\nexport default ResidualNetwork;\r\nfunction PostContent(){\r\n return(<Layout style={{ backgroundColor: 'white', padding: '0'}}>\r\n    <div>\r\n        <Tag color=\"blue\">Neural Network</Tag> <Tag color=\"blue\">Artificial Intelligence</Tag><Tag color=\"blue\">Machine Learning</Tag>\r\n    </div>\r\n    <Divider></Divider>\r\n<Paragraph>\r\n在深度学习中，两个严重影响了模型效果的问题是梯度消失问题与梯度下降问题。这两个问题的出现与深度学习的根本机制 - 反向传播损失函数梯度有关。在很长一段时间里，人们认为超过100层的网络是“不可训练”的。然而残差网络 (Residual Network, ResNet) 的出现改变了这一切。通过设计“短路”机制，残差网络可以让梯度更好的在网络的层之间传播，从而使得训练500+层的超深神经网络成为了可能。相似的机制也启发了一大批拥有shortcut connection的神经网络，例如在医学图像处理领域常见的 U-net 和 Dense Net。</Paragraph>\r\n\r\n<Title level={2}>深度残差网络 ResNet 解析</Title>\r\n<Paragraph><a href=\"https://arxiv.org/pdf/1512.03385.pdf\">论文链接 Deep Residual Learning for Image Recognition</a></Paragraph>\r\n<Divider></Divider>\r\n<Paragraph><Text strong>目录</Text></Paragraph>\r\n<ol>\r\n<li>什么是深度残差网络</li>\r\n<li>深度残差网络解决了什么问题</li>\r\n<li>深度残差网络的设计思路</li>\r\n<li>深度残差网络的表现</li>\r\n<li>深度残差网络与其他模型的交互</li>\r\n</ol>\r\n<Divider></Divider>\r\n<Title level={3}>1. 什么是深度残差网络</Title>\r\n<Paragraph>深度残差网络是 Kaiming He et al. 提出的，一种运用了短路连接的神经网络形式。深度残差网络本身并没有一个固定的结构与参数，这使得深度残差网络非常灵活，可以有效的插入其他模型而提高模型表现。</Paragraph>\r\n<Paragraph>像下图表示的一样，深度残差网络本质上是通过在卷积层之间插入短路连接来达到传播梯度的效果。</Paragraph>\r\n<Paragraph><Image alt=\"短路链接越过了中间的两层卷积\" src={`${PhotoLink}ResNet1.png`}width=\"25%\" style={{minWidth:\"250px\"}} fallback={FailImage} /></Paragraph>\r\n<Paragraph>短路链接在越过了卷积层后会直接与卷积层的输出结果进行对位相加(pointwise addition)。当反向传播执行时，一半的梯度会通过短路链接直接被传导到靠后的卷积层，另一半则会加上被短路链接越过的两个卷积层的参数梯度后再传播到靠后的卷积层。</Paragraph>\r\n<Paragraph>通过重复叠加这样的“残差网络块”就得到了深度残差网络。</Paragraph>\r\n<Title level={3}>2. 深度残差网络解决了什么问题</Title>\r\n<Paragraph>在深度残差网络提出前，所有超过50层的深度学习模型都会或多或少的受到梯度消失与梯度爆炸的影响。这两个问题对模型表现的影响具体表现为模型的准确率非常不稳定，有的时候模型准确率会因为梯度过小而几乎不改变，有的时候模型准确率会由于梯度过大而急剧降低。这使得大部分的网络只能拥有较少的层数。因为每一层卷积层相当于提取一次输入的特征，层数的限制也限制了模型提取复杂特征的能力。从下图中我们可以发现，虽然理论上更深的网络可以提取更加复杂的信息，实际实验中过深的网络一般会表现的比浅网络差。</Paragraph>\r\n<Paragraph><Image alt=\"短路链接越过了中间的两层卷积\" src={`${PhotoLink}ResNet2.png`} width=\"40%\" style={{minWidth:\"250px\"}} fallback={FailImage} /></Paragraph>\r\n<Paragraph>虽然一些神经网络模型通过在模型中间添加额外的损失函数进行反向传播来减少梯度消失与梯度爆炸的影响，但这并没有从本质上解决问题。</Paragraph>\r\n<Paragraph>在深度残差网络中，通过残差网络上的短接路径，梯度可以在非常深的网络中连贯的传播而不受到过多卷积层梯度的叠加。这从本质上避免了梯度消失与梯度爆炸问题。</Paragraph>\r\n<Title level={3}>3. 深度残差网络的设计思路</Title>\r\n<Paragraph>残差网络的设计思路非常简单：一个深层的网络不应该表现的比浅层网络更差。基于这个考量，作者在卷积层之间加上了短路链接。本质上，短路链接可以看作是一个恒等变换。如果在训练中，两个卷积层由于种种原因没能有效提取图片中的有效特征，因为有短路链接的存在，模型整体的效果也不会被影响很多。如果两个卷积层提取到了有用的特征，那么后面的层就可以采用这两个卷积层的结果从而提高模型的表现。换句话说，如果我们的目标函数是 <InlineMath math=\"H(x)\"/> 而输入为 <InlineMath math=\"x\"/>, 那么被短接的两个卷积层需要做的就是尽量拟合输入与目标函数之间的差距，也就是 <InlineMath math=\"H(x) - x\"/>。这也是为什么这种网络被称作“残差网络， Residual Net”的原因，每一层其实都在拟合当前输入与目标之间的差距值。</Paragraph>\r\n<Paragraph>一个有趣的现象是在人类的大脑中也有类似残差网络的结构出现在处理视觉信号的视觉神经中枢。在人脑的视觉神经中枢中神经元被分为5层，像一般的前馈神经网络一样，每一层都会接受前一层的处理结果并向后一层传输输出。一些研究发现，在第四层中的神经元会部分的直接与第一层中的神经元相连而跳过中间层。虽然人们目前还不完全清楚这样的结构在视觉神经中枢中的占比，但是这样的结构确实与深度残差网络结构不谋而合。[1]</Paragraph>\r\n<Paragraph><a href=\"https://en.wikipedia.org/wiki/Residual_neural_network\" class=\"LinkCard\"> Wikipedia: Residual Neural Network </a></Paragraph>\r\n<Title level={3}>4. 深度残差网络的表现</Title>\r\n<Paragraph>在论文中，作者分别使用了两种 Residual Block 来构建深度残差网络。一种被称作“building block”，这种block包含两个连续的<InlineMath math=\"3\times 3\"/> 卷积核，拥有较多的参数；而另一种被称作\"bottleneck building block\"，这种block包含三个卷积核，其中第一个和第三个是 <InlineMath math=\"1\times 1\"/>卷积核，中间的是 <InlineMath math=\"3\times 3\"/> 卷积核，这样的block相对于基本的basic block来说拥有更小的参数量。</Paragraph>\r\n<Paragraph><Image alt=\"Building Blocks of ResNet\" src={`${PhotoLink}ResNet5.png`} width=\"40%\" style={{minWidth:\"250px\"}} fallback={FailImage} /></Paragraph>\r\n<Paragraph>通过重复使用这两种block，搭配上合适的池化函数，作者构建了若干个深度不同的深度残差网络。这些网络分别有18, 34,  50, 101, 和 152层，一般被大家简称为 ResNet18, ResNet34, ..., ResNet152。同时，为了验证模型的短接通路对梯度传播的改善效果，作者还测试了非常极端的，拥有1202层的ResNet1202在CIFAR-10数据集上的表现。</Paragraph>\r\n<Paragraph><Image alt=\"ResNet 在 CIFAR-10数据集上的表现与前馈卷积网络的表现对比\" src={`${PhotoLink}ResNet3.png`} width=\"50%\" style={{minWidth:\"250px\"}} fallback={FailImage} /></Paragraph>\r\n<Paragraph>上图左侧表现了不同深度的简单卷积神经网络在数据集上的表现，我们可以发现简单的卷积神经网络随着深度的增加，错误率不降反增，表现最佳的Plain20神经网络在测试集上达到了10%的错误率。普通网络相比，在一般的深度范围内，深度残差网络的表现会随着深度的加深而提高。在中间的图中，我们可以看到深度残差网络达到了5%的错误率，比平常的前馈神经网络低了一半。</Paragraph>\r\n<Title level={3}>5. 深度残差网络与其他类似模型</Title>\r\n<Title level={4}>5.1 DenseNet</Title>\r\n<Paragraph>深度残差网络也启发了一些类似的网络类型，其中较为经典的包括 DenseNet， 在DenseNet中，短接链接的数量被增加了，每一个卷积层都会有一条专门的短接链接将自己的输出直接传播给较前的卷积层。</Paragraph>\r\n<Paragraph><Image alt=\"Dense Net Illustration\" src={`${PhotoLink}ResNet4.jpg`} width=\"25%\" style={{minWidth:\"250px\"}} fallback={FailImage} /></Paragraph>\r\n<Paragraph>因为DenseNet本质上包含了一个前馈神经网络的所有可能链接，所有的前馈神经网络都可以看作是DenseNet的一种特例来处理。这么多的链接也最大化了DenseNet本身进行特征提取和梯度传播的能力。</Paragraph>\r\n<Title level={4}>5.2 U-net</Title>\r\n<Paragraph><a href=\"https://markchenyutian.github.io/Markchen_Blog/2020/10/09/U-net.html\" class=\"LinkCard\">U-net 医学图像分割网络</a></Paragraph>\r\n<Paragraph>U-net 通过向下池化的操作来保证模型的健壮性，同时，通过skip connection将池化前的结果与膨胀卷积后的结果相拼接，模型可以保留输入图片在高分辨率下的一部分细节信息，这样的操作即提高了模型的健壮性，又不会输入图片内的大量细节。因为医学图像分割中常常会出现许多特征差异明显的正常&amp;非正常样本（对模型的健壮性又很大考验），同时需要模型对输入信息做出精确分割（需要精确计算病灶体积/截面积），U-net在这个这个细分领域内大展身手。</Paragraph>\r\n</Layout>\r\n);}","import React from 'react';\r\nimport '../../App.css';\r\nimport 'moment/locale/zh-cn';\r\nimport 'antd/dist/antd.css';\r\nimport '../../index.css';\r\nimport { Image, Layout, Typography, Tag, Divider } from 'antd';\r\nimport { InlineMath, BlockMath } from 'react-katex';\r\nimport 'katex/dist/katex.min.css';\r\nimport SyntaxHighlighter from 'react-syntax-highlighter';\r\nimport { lightfair } from 'react-syntax-highlighter/dist/esm/styles/hljs';\r\nimport AppHeader from '../../PublicComponent/Header';\r\nimport AppFooter from '../../PublicComponent/Footer';\r\nimport FailImage from '../../PublicComponent/FailImage';\r\nimport AppPageHeader from '../../PublicComponent/PageHeader';const { Title, Text, Paragraph } = Typography;\r\nconst { Content } = Layout;\r\nfunction WhatIsBayesNetwork(){\r\n   window.scrollTo(0,0);\r\n   return(\r\n       <Layout>\r\n           <AppHeader select='2'/>\r\n           <Content className='site-layout' style={{ padding: '0 24px', marginTop: 64 }}>\r\n           <AppPageHeader title='What Is Bayes Network'/>\r\n           <div className='site-layout-background' style={{ padding: 16 }}>\r\n              <PostContent/>\r\n           </div>\r\n           </Content>\r\n           <AppFooter/>\r\n        </Layout>\r\n   );\r\n}\r\nexport default WhatIsBayesNetwork;\r\nfunction PostContent(){\r\n return(\r\n<Layout style={{ backgroundColor: 'white', padding: '0'}}>\r\n<div>\r\n        <Tag color=\"blue\">Artificial Intelligence</Tag><Tag color=\"blue\">Machine Learning</Tag><Tag color=\"orange\">CS 188</Tag>\r\n</div>\r\n<Divider></Divider>\r\n<Paragraph>贝叶斯网络是人们在探索机器学习时的一个重要里程碑，通过贝叶斯网络，机器学习摆脱了以往基于形式逻辑推理和庞大知识库的限制，开始了“统计学习”的新纪元。那么什么是贝叶斯网络呢？贝叶斯网络和贝叶斯统计学派又有什么关系呢?</Paragraph>\r\n<Title level={2}>CS188 课堂笔记 - 贝叶斯网络 Bayesian Network</Title>\r\n<Paragraph><Text strong>目录</Text></Paragraph>\r\n<Paragraph type=\"secondary\">\r\n<ol>\r\n<li>统计贝叶斯学派与贝叶斯公式</li>\r\n<li>什么是贝叶斯网络</li>\r\n<li>为什么我们需要贝叶斯网络</li>\r\n<li>贝叶斯网络“加速”的原理</li>\r\n<li>用朴素的贝叶斯网络识别手写数字</li>\r\n<li>参考资料</li>\r\n</ol>\r\n</Paragraph>\r\n<Title level={3}>1 统计贝叶斯学派与贝叶斯公式</Title>\r\n<Paragraph>根据对统计的理解，数理统计存在<Text strong>概率学派</Text>与<Text strong>贝叶斯学派</Text>两种学派，他们之间的主要区别在于对于概率的理解方法不同</Paragraph>\r\n<Paragraph><Text strong>概率学派</Text>认为世界存在一个固定的<Text strong>先验概率</Text>，例如一枚公平的硬币抛出正反面的概率一定分别是1/2。换句话说，古典学派认为任何事件都<Text strong>存在一个固定的概率模型</Text>，虽然我们可能不知道这个概率分布中的一些参数，但是只要我们进行了足够多次的取样，我们可以通过取样的结果来推断事件的概率分布。</Paragraph>\r\n<Paragraph><Text strong>贝叶斯学派</Text>则认为世界<Text strong>没有一个确定的先验概率</Text>，假设我们只得到了事件A的样本X，那么我们就只能依靠样本X对事件A的概率分布做出推断，而不必考虑“可能出现但未出现（在样本X中）”的情况。在这种理解的背景下，我们每次对事件A进行采样就会更新我们对事件A各个情况概率分布的认知（Belief）。[1]</Paragraph>\r\n<Paragraph>两个学派都各有优势，在一些简单并可以做出大量模拟的情况（例如预测抛硬币正反的概率分布）下，概率学派可以较为精确的获得某一事件的发生概率；在一些难以分析，很难模拟/大量采样的情况（例如地震概率的预测）下，贝叶斯学派则有极大的优势，可以使用有限的信息帮助我们做出合理的推断</Paragraph>\r\n<Paragraph><Text strong>贝叶斯公式</Text>是贝叶斯学派的重要理论之一，这个公式告诉了我们如何通过我们对事件A已有的认知和新的采样（evidence）B来更新事件A的<Text strong>后验概率</Text>（在事件B发生后我们对事件A概率分布的新认知）\r\n<BlockMath math=\"\r\nP(A \\mid B) = \frac{P(B\\mid A)P(A)}{P(B)}\r\n\"/>\r\n概率学派也有关于这个公式的另一套解释方法：概率学派将概率看为“结果的比例（结果A在所有结果中的概率记为P(A) ）。这个公式在这种解释下成为了描述“具有B的所有结果中有A性质的结果所占的比例 = 具有A性质的所有结果中具有B性质的结果所占的比例 <InlineMath math=\"\times\"/> 所有结果中A的比例 / 所有结果中B的比例” [2]</Paragraph>\r\n<Title level={3}>2 什么是贝叶斯网络</Title>\r\n<Paragraph>贝叶斯网络是一种描述<Text strong>随机变量之间互相条件独立关系的有向无环图</Text>。在这个有向无环图中，每个节点代表一个<Text strong>随机变量对其父节点的条件概率分布</Text> <InlineMath math=\"P(X_i \\mid parents(X_i))\"/>，每一条边可以理解成变量之间的联系。</Paragraph>\r\n<Paragraph type=\"secondary\">\r\n<Paragraph>注意：虽然一般来讲这种“联系”可以被解释为“因果关系”，但是实际上 这种关系并不一定是因果关系，只要两个变量之间互相不条件独立就应该被连在一起</Paragraph>\r\n</Paragraph>\r\n<Paragraph>在贝叶斯网络中，已知<InlineMath math=\"X\"/>的父节点<InlineMath math=\"parents(X)\"/>时<InlineMath math=\"X\"/>的条件概率分布<InlineMath math=\"P(X\\mid parents(X))\"/> 与已知<InlineMath math=\"X\"/>的父节点时网络中<InlineMath math=\"X\"/>的“祖先节点”的概率分布<InlineMath math=\"P(ancestor(X)\\mid parents(X))\"/>互相条件独立。[3]</Paragraph>\r\n<Paragraph type=\"secondary\">\r\n<Paragraph>例子：<Image src=\"https://markchenyutian.github.io/Markchen_Blog/Asset/Bayes3.png\" alt=\"image-20200430104043594\" width=\"200px\" fallback={FailImage} /></Paragraph>\r\n<Paragraph>在这样一给贝叶斯网络图中，E的父节点<InlineMath math=\"parent(E) = {D}\"/>，E不包括父节点的祖先节点<InlineMath math=\"ancestor(E) = {B, C, A}\"/></Paragraph>\r\n<Paragraph>通过贝叶斯网络的定义，我们可以知道随机变量C, D, E之间存在这样的关系：</Paragraph>\r\n</Paragraph>\r\n<Paragraph><BlockMath math=\"\r\nP(E\\mid D) \\perp P(C\\mid D)\r\n\"/></Paragraph>\r\n<Paragraph type=\"secondary\">\r\n<Paragraph>也就是说</Paragraph>\r\n</Paragraph>\r\n<Paragraph><BlockMath math=\"\r\nP(E\\mid D, B, C, A) = P(E\\mid D)\r\n\"/></Paragraph>\r\n<Paragraph>贝叶斯网络本质上只是一种维持子节点与其祖先节点（不包括父节点）在给定父节点的条件下互相条件独立的存储随机变量之间互相关系的数据结构。</Paragraph>\r\n<Title level={3}>3. 为什么我们需要贝叶斯网络</Title>\r\n<Paragraph>在生产生活中，我们经常需要对具有随机性的状态的出现概率进行推断。假设我们想用随机变量<InlineMath math=\"X_0\"/>到<InlineMath math=\"X_n\"/>来表示一个事件的“状态”，其中每一个随机变量都只有2个可能的取值：1（发生）或0（不发生） 我们这时候想要得知<InlineMath math=\"P(x_1, x_2, \\cdots, x_n)\"/>这的概率分布。</Paragraph>\r\n<Paragraph>如果使用直接列出一张全联合分布的概率分布表（如下）的话，整张概率分布表会有<InlineMath math=\"2^n\"/>行，每次计算一行都要计算随机变量的所有情况，这使得求解概率分布的时间复杂度极高（时间复杂度<InlineMath math=\"O(n2^n)\"/>）。</Paragraph>\r\n<Paragraph><Image src=\"https://markchenyutian.github.io/Markchen_Blog/Asset/Bayes2.png\" alt=\"image-20200430112420798\" width=\"200px\" fallback={FailImage} /></Paragraph>\r\n<Paragraph>如果我把这n个随机变量用贝叶斯网络表示出来，因为贝叶斯网络可以很好的表达随机变量之间的相互条件独立关系，我们的计算量可以大大减小\r\n<BlockMath math=\"\r\nP(x_1, x_2, \\cdots, x_n) = \\prod^n_{i = 1}{P(x_i | x_{i+1}, x_{i + 2}, \\cdots x_n)}\r\n\"/>\r\n上面的式子中连乘号中的概率分布也可以表示为：\r\n<BlockMath math=\"\r\nP(x_i \\mid parents(x_i), ancestor(x_i))\r\n\"/>\r\n因为我们知道 <InlineMath math=\"P(ancestor(x_i)\\mid parents(x_i))\\perp P(x_i \\mid parents(x_i))\"/>，我们可以消去上式中给定条件里的<InlineMath math=\"ancestor(x_i)\"/>这一项</Paragraph>\r\n<Paragraph>这时候，我们可以得知：\r\n<BlockMath math=\"\r\nP(x_1, x_2, \\cdots, x_n) = \\prod^n_{i = 1}P(x_i \\mid parents(x_i))\r\n\"/>\r\n看到这里，你可能会觉得这个式子有些似曾相识……</Paragraph>\r\n<Paragraph>是的，这里被连乘的每一项就是贝叶斯网络中每一个节点所存储的条件概率表所存储的概率分布！</Paragraph>\r\n<Paragraph>这时候，在一个最多有<InlineMath math=\"k\"/>个父节点的贝叶斯网络中，求解状态的概率分布所需要的时间复杂度就被缩小到了<InlineMath math=\"O(n2^k)\"/>。虽然求解问题依然是一个非多项式时间问题（NP）， 但是在大多数情况中贝叶斯网络的使用可以有效的降低时间复杂度的幂。[3]</Paragraph>\r\n<Title level={3}>4. 贝叶斯网络“加速”的原理</Title>\r\n<Paragraph>为什么贝叶斯网络处理同样的问题比直接计算所有随机变量的全联合分布要快呢？这个问题其实可以在贝叶斯网络的时间复杂度表达式中看出端倪：对于一个<Text strong>每个节点最多有<InlineMath math=\"k\"/>个父节点</Text>的贝叶斯网络，求解概率分布的时间复杂度是<InlineMath math=\"O(n2^k)\"/>。这意味着如果有一个贝叶斯网络是一个<Text strong>完全图</Text>（每个节点之间都有连线）的话，它的求解时间复杂度会达到<InlineMath math=\"O(n2^n)\"/>，和全联合分布一样。</Paragraph>\r\n<Paragraph>实际上，贝叶斯网络可以计算的比全联合分布快是因为贝叶斯网络可以有效的表示变量之间的条件独立关系，基于这种条件独立的假设来简化计算，从而降低算法时间复杂度。</Paragraph>\r\n<Title level={3}>5. 朴素的贝叶斯网络识别手写数字</Title>\r\n<Paragraph>上面简单的介绍了什么是贝叶斯网络和贝叶斯网络的存在意义，接下来我们要看一个<Text strong>朴素的</Text>贝叶斯网络用来识别MNIST数据集中的手写数字的一个实践案例</Paragraph>\r\n<Title level={4}>5.0 什么是MNIST数据集</Title>\r\n<Paragraph>MNIST数据集是美国国家标准与技术研究所收集整理标注的一个手写数字数据集，其中包括了60000张28*28的8bit灰度手写数字图片作为训练集，还有10000张28*28的8bit灰度图片作为测试集。每张图片由<InlineMath math=\"28^2 = 784\"/>个像素构成，每个像素取值（从白到黑）在<InlineMath math=\"[0, 255]\"/>的范围内。</Paragraph>\r\n<Paragraph><Image alt=\"MNIST数据集中的一张'4'的样本\" src=\"https://markchenyutian.github.io/Markchen_Blog/Asset/Bayes4.png\" width=\"200px\" fallback={FailImage} /></Paragraph>\r\n<Paragraph>每一张图像都有一个“标签”，这个标签代表着这个图片上写的数字。</Paragraph>\r\n<Title level={4}>5.1 为什么说是“朴素的”贝叶斯网络</Title>\r\n<Paragraph>因为在这个例子里面，我们假定每一个像素是一个单独的feature（特性），并且我们认为所有的像素之间都是互相独立的（显然不是，一个高亮的像素周边的像素大概率也比较亮）。这样一个略微脱离实际的假设使得我们可以大大简化模型的贝叶斯网络并且可以极快的求解概率分布（因为每个像素都只有一个父节点——图像的标签）。</Paragraph>\r\n<Paragraph><Image src=\"https://markchenyutian.github.io/Markchen_Blog/Asset/Bayes1.png\" width=\"200px\" fallback={FailImage} /></Paragraph>\r\n<Title level={4}>5.2 如何运用模型预测</Title>\r\n<Paragraph>对于一张给定的图片，我们把里面的784个像素看成784个随机变量，同时，我们记这个模型的标签为<InlineMath math=\"label\"/>，在这种设定下，一张图片的标签可以这样表示：\r\n<BlockMath math=\"\r\nlabel = {\\underset {label\\in [0, 9]}{\\operatorname {arg\\,max} }}\\,(P(label, f_1, f_2, \\cdots, f_{784}))\r\n\"/></Paragraph>\r\n<Paragraph type=\"secondary\">\r\n<Paragraph>对于一幅图片（给定<InlineMath math=\"f_1, f_2, \\cdots f_{784}\"/>），我们希望找到一个0 - 9之间的label，使得<InlineMath math=\"P(label, f_1, f_2, \\cdots, f_{784})\"/>的值最大</Paragraph>\r\n</Paragraph>\r\n<Paragraph>使用贝叶斯网络，我们可以发现这个概率<InlineMath math=\"P(label, f_1, f_2, \\cdots, f_{784})\"/>可以这么计算：\r\n<BlockMath math=\"\r\nP(label, f_1, f_2, \\cdots, f_{784}) = P(label) \\cdot P(f_1 \\mid label) \\cdot P(f_2\\mid label)\\cdots P(f_{784}\\mid label)\r\n\"/>\r\n我们只用完成这样的一个简单运算就可以得到一张照片是label = A的概率了：</Paragraph>\r\n<SyntaxHighlighter language=\"python\" style={lightfair}\r\n        children={\r\n            `\r\ndef predict(data):\r\n    global LabelDistribution, PixelDistribution, THRESHOLD\r\n    labelProbTable = [1] * 10\r\n    for i in range(10):\r\n        labelProbTable[i] <em>= LabelDistribution[i]\r\n        for pixel in range(len(data)):\r\n            if data[pixel] >= THRESHOLD: labelProbTable[i] </em>= PixelDistribution[i][pixel]\r\n            else: labelProbTable[i] *= (1 - PixelDistribution[i][pixel])\r\n\r\n    MAX_PROB, MAX_LABEL = -1, -1\r\n    for i in range(10):\r\n        if labelProbTable[i] > MAX_PROB:\r\n            MAX_PROB = labelProbTable[i]\r\n            MAX_LABEL = i\r\n    return MAX_LABEL\r\n`}/>\r\n<Title level={4}>5.3 如何训练模型</Title>\r\n<Paragraph>通过上面的公式，我们知道可以很方便的使用<InlineMath math=\"P(label)\"/>和<InlineMath math=\"P(f_i\\mid label)\"/>来预测一张图片的标签，那么我们怎么获得这两种数据呢?答案是：</Paragraph>\r\n<Paragraph><Text strong>数数</Text></Paragraph>\r\n<Paragraph>是的，这个”天真烂漫的朴素贝叶斯网络“的整个训练过程只在做一件事情：数数</Paragraph>\r\n<Paragraph>我们通过统计训练数据里60000个标签的概率分布来得到<InlineMath math=\"P(label)\"/>，同时我们对每一张图片的每一个像素进行统计，如果像素亮度超过阈值就记+1上去，最后得到每一个标签下所有像素超过阈值的概率，也就是<InlineMath math=\"P(f_i = 1\\mid label)\"/>。</Paragraph>\r\n\r\n<SyntaxHighlighter language=\"python\" style={lightfair}\r\n        children={\r\n            `\r\nData = [list(map(int, line.strip().split(\",\"))) for line in open(\"mnist_train.csv\").read().strip().split(\"\\n\")]\r\nLabelDistribution = {i: 0 for i in range(10)}\r\nPixelDistribution = [{i: 0 for i in range(784)} for _ in range(10)]\r\nTHRESHOLD = 100\r\n\r\n# Calculate the P(F_i | y)\r\n\r\ndef train():\r\n    global LabelDistribution, PixelDistribution, Data, THRESHOLD\r\n    for line in Data:\r\n        label = line[0]\r\n        pixels = line[1:]\r\n        LabelDistribution[label] += 1\r\n        for pixel in range(len(pixels)):\r\n            if pixels[pixel] &gt;= THRESHOLD:\r\n                PixelDistribution[label][pixel] += 1\r\n    for i in range(10): normalize(PixelDistribution[i], label=i)\r\n        normalize(LabelDistribution)`\r\n}/>\r\n<Title level={4}>5.4 模型准确率</Title>\r\n<Paragraph>虽然这个模型看上去非常的不靠谱（假设所有feature相互独立），但是竟然可以达到高达84.4%的分类准确率！(当然，比起其他像神经网络一样的fancy方法，这个结果也很<Text strong>朴素</Text>）</Paragraph>\r\n<Title level={3}>6. 参考资料</Title>\r\n<Paragraph>[1]:  “hgz_dm.” <em>统计学中的频率学派与贝叶斯学派 - hgz_dm - 博客园</em>, www.cnblogs.com/hgz-dm/p/10292949.html.</Paragraph>\r\n<Paragraph>[2]: “Bayes' Theorem.” <em>Wikipedia</em>, Wikimedia Foundation, 27 Apr. 2020, en.wikipedia.org/wiki/Bayes'_theorem.</Paragraph>\r\n<Paragraph>[3]: Russell, Stuart J. <em>Artificial Intelligence: a Modern Approach</em>. Pearson, 2016.</Paragraph>\r\n</Layout>\r\n);}","import React from 'react';\r\nimport {HashRouter, Route, Switch} from 'react-router-dom';\r\n\r\nimport MainContent from './Main/MainPage';\r\nimport MainPost from './Posts/PostsMain';\r\nimport MainNotes from './Notes/NotesMain';\r\n\r\nimport HowDoNeuralNetworkWork from './Posts/MyPosts/HowDoNeuralNetworkWork';\r\nimport WhatIsLSTM from './Posts/MyPosts/WhatIsLSTM';\r\nimport ResidualNetwork from './Posts/MyPosts/ResidualNetwork';\r\nimport WhatIsBayesNetwork from './Posts/MyPosts/WhatIsBayesNetwork';\r\n\r\nconst BasicRoute = () => (\r\n    <HashRouter>\r\n        <Switch>\r\n            <Route exact path=\"/\" component={MainContent}/>\r\n            <Route exact path=\"/posts\" component={MainPost}/>\r\n            <Route exact path=\"/notes\" component={MainNotes}/>\r\n\r\n            <Route exact path=\"/posts/HowDoNeuralNetworkWork\" component={HowDoNeuralNetworkWork}/>\r\n            <Route exact path=\"/posts/WhatIsLSTM\" component={WhatIsLSTM}/>\r\n            <Route exact path=\"/posts/ResidualNetwork\" component={ResidualNetwork}/>\r\n            <Route exact path=\"/posts/WhatIsBayesNetwork\" component={WhatIsBayesNetwork}/>\r\n        </Switch>\r\n    </HashRouter>\r\n);\r\n\r\n\r\nexport default BasicRoute;","import './App.css';\nimport BasicRoute from './Router';\n\nfunction App() {\n  return (\n    <BasicRoute/>\n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}